{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1697361847933,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "PKiDhRvyNwiw",
    "outputId": "166b32ab-cf89-4855-854e-283c05ec1158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/drive/MyDrive/Mushroom-Classifier'\n",
      "/home/broug/Desktop/Mushroom-Classifier/training\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    if os.environ['COLAB_RELEASE_TAG']:\n",
    "        print(\"Found Colab Environment\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        from google.colab import auth\n",
    "        auth.authenticate_user()\n",
    "\n",
    "        %pip install -q tensorflow==2.10.0 wandb python-dotenv tensorboard_plugin_profile tensorflow_io==0.27.0\n",
    "        exit()\n",
    "    elif Path().cwd().name == 'Mushroom-Classifier':\n",
    "        print(\"Found Other Environment\")\n",
    "        %pip install -q tensorflow==2.10.0 wandb python-dotenv tensorboard_plugin_profile tensorflow_io==0.27.0\n",
    "        exit()\n",
    "    else:\n",
    "        print('Please run this notebook from the root of the repository')\n",
    "        exit()\n",
    "\n",
    "%cd /content/drive/MyDrive/Mushroom-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1697361848140,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "GEQKIS-h_JS2"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3230,
     "status": "ok",
     "timestamp": 1697361852221,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "LfAu_aLnNo7l",
    "outputId": "9eb39522-b6f7-46bf-e0e8-74635f093a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.14.0\n"
     ]
    }
   ],
   "source": [
    "import math, re, os, pickle\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback, WandbModelCheckpoint\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from src.models.swintransformer import SwinTransformer\n",
    "# from src.optimizers import lion\n",
    "# from prefect import task, flow\n",
    "\n",
    "print(f\"Tensorflow version {tf.__version__}\")\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "np.set_printoptions(threshold=15, linewidth=80)\n",
    "\n",
    "from config import GCFG, CFG\n",
    "\n",
    "CFG2 = GCFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 10757,
     "status": "ok",
     "timestamp": 1697361887542,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "_-bkudXcZiWd",
    "outputId": "1c194518-0412-4424-8e1b-f358bd232512"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-broughton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Mushroom-Classifier/training/wandb/run-20231026_122647-1gj7898x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/1gj7898x' target=\"_blank\">skilled-leaf-80</a></strong> to <a href='https://wandb.ai/g-broughton/Mushroom-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/Mushroom-Classifier' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/1gj7898x' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier/runs/1gj7898x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/1gj7898x?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa9fa78a5c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_time = datetime.now().strftime('%m%d-%H%M')\n",
    "log_dir = f\"{CFG2.GCS_REPO}/logs/{CFG2.MODEL}/{save_time}\"\n",
    "\n",
    "# wandb.tensorboard.patch(root_logdir=log_dir + \"/tf\")\n",
    "wandb.init(project=\"Mushroom-Classifier\", tags=[CFG2.MODEL, CFG2.OPT, CFG2.LR_SCHED, str(CFG2.IMAGE_SIZE[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TPU_NAME=local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 13:26:08.078897: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5623a163ea00 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-26 13:26:08.078939: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.078950: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.078960: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.078969: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.078979: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.078988: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.078998: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.079007: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): TPU, 2a886c8\n",
      "2023-10-26 13:26:08.079150: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079209: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079266: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079329: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079384: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079564: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079617: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079668: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079744: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079804: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.079956: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080028: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080082: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080148: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080227: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080421: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080473: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080528: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080594: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080645: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080813: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080868: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080927: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.080990: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081043: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081220: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081273: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081333: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081384: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081444: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081628: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081685: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081743: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081798: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.081853: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.082044: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.082102: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.082169: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.082220: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:26:08.082274: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "Number of accelerators:  8\n"
     ]
    }
   ],
   "source": [
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "\n",
    "CFG2.REPLICAS = strategy.num_replicas_in_sync\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 13:32:57.303378: I tensorflow/compiler/xla/stream_executor/tpu/tpu_initializer_helper.cc:242] Libtpu path is: libtpu.so\n",
      "D1026 13:32:57.467209113   45943 config.cc:175]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D1026 13:32:57.467235345   45943 config.cc:175]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D1026 13:32:57.467240557   45943 config.cc:175]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D1026 13:32:57.467244820   45943 config.cc:175]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D1026 13:32:57.467249466   45943 config.cc:175]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D1026 13:32:57.467253614   45943 config.cc:175]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D1026 13:32:57.467257816   45943 config.cc:175]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D1026 13:32:57.467261795   45943 config.cc:175]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D1026 13:32:57.467265882   45943 config.cc:175]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D1026 13:32:57.467269816   45943 config.cc:175]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D1026 13:32:57.467273823   45943 config.cc:175]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D1026 13:32:57.467277757   45943 config.cc:175]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "D1026 13:32:57.467281819   45943 config.cc:175]                        gRPC EXPERIMENT schedule_cancellation_over_write    OFF (default:OFF)\n",
      "D1026 13:32:57.467285779   45943 config.cc:175]                        gRPC EXPERIMENT trace_record_callops                OFF (default:OFF)\n",
      "D1026 13:32:57.467289733   45943 config.cc:175]                        gRPC EXPERIMENT event_engine_dns                    OFF (default:OFF)\n",
      "D1026 13:32:57.467293907   45943 config.cc:175]                        gRPC EXPERIMENT work_stealing                       OFF (default:OFF)\n",
      "D1026 13:32:57.467297982   45943 config.cc:175]                        gRPC EXPERIMENT client_privacy                      ON  (default:ON)\n",
      "D1026 13:32:57.467301953   45943 config.cc:175]                        gRPC EXPERIMENT canary_client_privacy               ON  (default:ON)\n",
      "D1026 13:32:57.467305902   45943 config.cc:175]                        gRPC EXPERIMENT server_privacy                      ON  (default:ON)\n",
      "D1026 13:32:57.467309838   45943 config.cc:175]                        gRPC EXPERIMENT unique_metadata_strings             OFF (default:OFF)\n",
      "D1026 13:32:57.467313816   45943 config.cc:175]                        gRPC EXPERIMENT keepalive_fix                       OFF (default:OFF)\n",
      "D1026 13:32:57.467317793   45943 config.cc:175]                        gRPC EXPERIMENT keepalive_server_fix                ON  (default:ON)\n",
      "D1026 13:32:57.467321770   45943 config.cc:175]                        gRPC EXPERIMENT promise_based_google_auth_filter    ON  (default:ON)\n",
      "D1026 13:32:57.467325762   45943 config.cc:175]                        gRPC EXPERIMENT promise_based_client_bwe_delegation OFF (default:OFF)\n",
      "D1026 13:32:57.467329749   45943 config.cc:175]                        gRPC EXPERIMENT promise_based_server_bwe_delegation OFF (default:OFF)\n",
      "D1026 13:32:57.467333739   45943 config.cc:175]                        gRPC EXPERIMENT oss_binary_logging                  OFF (default:OFF)\n",
      "D1026 13:32:57.467337697   45943 config.cc:175]                        gRPC EXPERIMENT loas_prefer_rekey_next_protocol     OFF (default:OFF)\n",
      "I1026 13:32:57.467515862   45943 ev_epoll1_linux.cc:123]               grpc epoll fd: 64\n",
      "D1026 13:32:57.467533033   45943 ev_posix.cc:113]                      Using polling engine: epoll1\n",
      "D1026 13:32:57.467838467   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"priority_experimental\"\n",
      "D1026 13:32:57.467847702   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D1026 13:32:57.467857647   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D1026 13:32:57.467868230   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"pick_first\"\n",
      "D1026 13:32:57.467873812   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"round_robin\"\n",
      "D1026 13:32:57.467878897   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"weighted_round_robin\"\n",
      "D1026 13:32:57.467903130   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"grpclb\"\n",
      "D1026 13:32:57.467923032   45943 dns_resolver_plugin.cc:44]            Using ares dns resolver\n",
      "D1026 13:32:57.467945813   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"rls_experimental\"\n",
      "D1026 13:32:57.467973976   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D1026 13:32:57.467979230   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D1026 13:32:57.467984681   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"cds_experimental\"\n",
      "D1026 13:32:57.467989840   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D1026 13:32:57.467994687   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D1026 13:32:57.467999876   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D1026 13:32:57.468005142   45943 lb_policy_registry.cc:47]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D1026 13:32:57.468010700   45943 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\n",
      "I1026 13:32:57.468824022   45943 ev_epoll1_linux.cc:360]               grpc epoll fd: 66\n",
      "I1026 13:32:57.493331636   45943 socket_utils_common_posix.cc:366]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "2023-10-26 13:32:57.534783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-broughton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Mushroom-Classifier/training/wandb/run-20231026_133304-alxxhy8r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/alxxhy8r' target=\"_blank\">volcanic-monkey-86</a></strong> to <a href='https://wandb.ai/g-broughton/Mushroom-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/Mushroom-Classifier' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/alxxhy8r' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier/runs/alxxhy8r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 13:33:09.995469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563d7e8ddcc0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-26 13:33:09.995506: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995518: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995527: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995537: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995546: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995556: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995570: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995579: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): TPU, 2a886c8\n",
      "2023-10-26 13:33:09.995713: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.995790: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.995849: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.995909: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.995957: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996141: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996221: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996286: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996353: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996402: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996587: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996662: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996721: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996784: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.996834: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.997004: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.997062: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.997129: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.997190: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:09.997249: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.001982: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002047: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002099: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002162: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002223: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002389: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002450: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002551: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002620: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002676: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002847: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002915: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.002974: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.003033: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.003086: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.003273: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.003334: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.003406: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.003458: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:10.003522: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 13:33:14.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mNumber of accelerators: \u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:14.840\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mTraining data shapes:\u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:16.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1m((64, 3, 224, 224), (64,))\u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:17.031\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1m((64, 3, 224, 224), (64,))\u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:17.168\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1m((64, 3, 224, 224), (64,))\u001b[0m\n",
      "2023-10-26 13:33:17.253502: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "\u001b[32m2023-10-26 13:33:18.390\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m53\u001b[0m - \u001b[34m\u001b[1mTraining data label examples: [154  41 146 ...  59 421 421]\u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:18.392\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mValidation data shapes:\u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:19.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m56\u001b[0m - \u001b[34m\u001b[1m((64, 3, 224, 224), (64,))\u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:19.040\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m56\u001b[0m - \u001b[34m\u001b[1m((64, 3, 224, 224), (64,))\u001b[0m\n",
      "2023-10-26 13:33:19.043626: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "\u001b[32m2023-10-26 13:33:19.082\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m56\u001b[0m - \u001b[34m\u001b[1m((64, 3, 224, 224), (64,))\u001b[0m\n",
      "\u001b[32m2023-10-26 13:33:19.200\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m57\u001b[0m - \u001b[34m\u001b[1mValidation data label examples: [ 61 269 229 ...   5 445  89]\u001b[0m\n",
      "2023-10-26 13:33:19.219757: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-26 13:33:19.265547: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-10-26 13:33:20.645208: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 13:33:20.645293: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/home/broug/Mushroom-Classifier/training/base_models/swin_large_224/base_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Mushroom-Classifier/training/train.py:263\u001b[0m\n\u001b[1;32m    260\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mTPUStrategy(cluster_resolver)\n\u001b[1;32m    261\u001b[0m replicas \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync\n\u001b[0;32m--> 263\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Mushroom-Classifier/training/train.py:66\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(CFG2, CFG, replicas)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# # Peek at training data\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# training_dataset = tr_fn.get_training_dataset(TRAINING_FILENAMES, CFG)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# training_dataset = training_dataset.unbatch().batch(20)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# train_batch = iter(training_dataset)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# training_viz.display_batch_of_images(next(train_batch), class_dict)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m---> 66\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtr_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     opt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.0001\u001b[39m)  \u001b[38;5;66;03m# tr_fn.create_optimizer(CFG)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy()\n",
      "File \u001b[0;32m~/Mushroom-Classifier/training/src/training/NN.py:37\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(CFG, class_dict)\u001b[0m\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m SwinTransformer(CFG\u001b[38;5;241m.\u001b[39mMODEL, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(class_dict), include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m     33\u001b[0m     img_adjust_layer,\n\u001b[1;32m     34\u001b[0m     model,\n\u001b[1;32m     35\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mlen\u001b[39m(class_dict), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m ])\n\u001b[0;32m---> 37\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_models\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/home/broug/Mushroom-Classifier/training/base_models/swin_large_224/base_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "%run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30289,
     "status": "ok",
     "timestamp": 1697361917827,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "PBI-WAppNo7s",
    "outputId": "955d1b29-9dfb-45aa-dc46-9cbd9c462c7e"
   },
   "outputs": [],
   "source": [
    "# # Detect hardware\n",
    "# try:\n",
    "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "# except ValueError:  # If TPU not found\n",
    "#     tpu = None\n",
    "\n",
    "# if tpu:\n",
    "#     tf.config.experimental_connect_to_cluster(tpu)\n",
    "#     tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#     strategy = tf.distribute.TPUStrategy(tpu)\n",
    "# else:\n",
    "#     strategy = tf.distribute.get_strategy()\n",
    "# CFG2.REPLICAS = strategy.num_replicas_in_sync\n",
    "# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnHHCJk-No7v"
   },
   "source": [
    "## Visualization Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1697361918383,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "yTKGP-V2No7x"
   },
   "outputs": [],
   "source": [
    "def batch_to_numpy_images_and_labels(data):\n",
    "    images, labels = data\n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n",
    "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
    "    # If no labels, only image IDs, return None for labels (this is the case for test data)\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "    if correct_label is None:\n",
    "        return class_dict[label], True\n",
    "    correct = (label == correct_label)\n",
    "    return \"{} [{}{}{}]\".format(class_dict[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
    "                                class_dict[correct_label] if not correct else ''), correct\n",
    "\n",
    "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "def display_batch_of_images(databatch, predictions=None):\n",
    "    \"\"\"This will work with:\n",
    "    display_batch_of_images(images)\n",
    "    display_batch_of_images(images, predictions)\n",
    "    display_batch_of_images((images, labels))\n",
    "    display_batch_of_images((images, labels), predictions)\n",
    "    \"\"\"\n",
    "    # data\n",
    "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
    "    if labels is None:\n",
    "        labels = [None for _ in enumerate(images)]\n",
    "\n",
    "    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images)//rows\n",
    "\n",
    "    # size and spacing\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols,1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "\n",
    "    # display\n",
    "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
    "        title = '' if label is None else class_dict[label]\n",
    "        correct = True\n",
    "        if predictions is not None:\n",
    "            title, correct = title_from_label_and_target(predictions[i], label)\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
    "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
    "\n",
    "    #layout\n",
    "    plt.tight_layout()\n",
    "    if label is None and predictions is None:\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    else:\n",
    "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()\n",
    "\n",
    "def display_confusion_matrix(cmat, score, precision, recall):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(cmat, cmap='Reds')\n",
    "    ax.set_xticks(range(len(class_dict)))\n",
    "    ax.set_xticklabels(class_dict, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    ax.set_yticks(range(len(class_dict)))\n",
    "    ax.set_yticklabels(class_dict, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    titlestring = \"\"\n",
    "    if score is not None:\n",
    "        titlestring += 'f1 = {:.3f} '.format(score)\n",
    "    if precision is not None:\n",
    "        titlestring += '\\nprecision = {:.3f} '.format(precision)\n",
    "    if recall is not None:\n",
    "        titlestring += '\\nrecall = {:.3f} '.format(recall)\n",
    "    if titlestring != \"\":\n",
    "        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n",
    "    plt.show()\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title(f'model {title}')\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])\n",
    "    wandb.log({\"chart\": plt})\n",
    "    path = CFG.ROOT / \"images\" / CFG.MODEL\n",
    "    path.mkdir(exist_ok=True)\n",
    "    plt.savefig(path / f'{title}-{save_time}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1697361919060,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "8ynfISf3No7z"
   },
   "outputs": [],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)  # image format uint8 [0,255]\n",
    "    image = tf.reshape(image, [*CFG.IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'dataset': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'longitude': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'latitude': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'norm_date': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'class_priors': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'class_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class_id'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(CFG.BATCH_SIZE * 10)\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO) # if labeled else read_unlabeled_tfrecord\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def data_augment(image, label):\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
    "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    # image = tf.image.random_flip_left_right(image)\n",
    "    #image = tf.image.random_saturation(image, 0, 2)\n",
    "    return image, label\n",
    "\n",
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "     # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.batch(CFG.BATCH_SIZE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(CFG.BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1697361919818,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "_T0iLwTwNo71"
   },
   "outputs": [],
   "source": [
    "GCS_PATH_SELECT = {\n",
    "    192: f'{CFG2.GCS_REPO}/tfrecords-jpeg-192x192',\n",
    "    224: f'{CFG2.GCS_REPO}/tfrecords-jpeg-224x224v2',\n",
    "    384: f'{CFG2.GCS_REPO}/tfrecords-jpeg-384x384',\n",
    "    512: f'{CFG2.GCS_REPO}/tfrecords-jpeg-512x512',\n",
    "}\n",
    "GCS_PATH = GCS_PATH_SELECT[CFG2.IMAGE_SIZE[0]]\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(f'{GCS_PATH}/train*.tfrec')\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(f'{GCS_PATH}/val*.tfrec')\n",
    "\n",
    "class_dict = pickle.load(open('src/class_dict.pkl', 'rb'))\n",
    "\n",
    "CFG2.NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "CFG2.NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
    "\n",
    "CFG = CFG(REPLICAS=CFG2.REPLICAS, NUM_TRAINING_IMAGES=CFG2.NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES=CFG2.NUM_VALIDATION_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cC5l84OONo73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes:\n",
      "(64, 224, 224, 3) (64,)\n",
      "(64, 224, 224, 3) (64,)\n",
      "(64, 224, 224, 3) (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:27:22.276860: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label examples: [222 445 194 ... 279 311 271]\n",
      "Validation data shapes:\n",
      "(64, 224, 224, 3) (64,)\n",
      "(64, 224, 224, 3) (64,)\n",
      "(64, 224, 224, 3) (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:27:24.545559: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data label examples: [ 97 221 410 ...  84 177 246]\n"
     ]
    }
   ],
   "source": [
    "# data dump\n",
    "print(\"Training data shapes:\")\n",
    "for image, label in get_training_dataset().take(3):\n",
    "    print(image.numpy().shape, label.numpy().shape)\n",
    "print(\"Training data label examples:\", label.numpy())\n",
    "print(\"Validation data shapes:\")\n",
    "for image, label in get_validation_dataset().take(3):\n",
    "    print(image.numpy().shape, label.numpy().shape)\n",
    "print(\"Validation data label examples:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SYJ_luSVNo74"
   },
   "outputs": [],
   "source": [
    "# Peek at training data\n",
    "training_dataset = get_training_dataset()\n",
    "training_dataset = training_dataset.unbatch().batch(20)\n",
    "train_batch = iter(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvzhBMQ_No75"
   },
   "outputs": [],
   "source": [
    "# run this cell again for next set of images\n",
    "# display_batch_of_images(next(train_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1_Bez6dNo76"
   },
   "source": [
    "you can select from these models:\n",
    "- swin_tiny_224\n",
    "- swin_small_224\n",
    "- swin_base_224\n",
    "- swin_base_384\n",
    "- swin_large_224\n",
    "- swin_large_384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1697362294835,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "6V4wIMBH5xPK"
   },
   "outputs": [],
   "source": [
    "def make_callbacks(CFG):\n",
    "    # options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "    options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_sparse_categorical_crossentropy\",\n",
    "            patience=CFG.ES_PATIENCE,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        # tf.keras.callbacks.TensorBoard(log_dir=log_dir + \"/tf\", profile_batch=(50, 250)),\n",
    "        tf.keras.callbacks.CSVLogger(\n",
    "            filename=f'{CFG.GCS_REPO}/logs/{save_time}-csv_log.csv',\n",
    "            separator=\",\",\n",
    "            append=False,\n",
    "        ),\n",
    "        # wandb.keras.WandbMetricsLogger(log_freq='batch'),\n",
    "        # wandb.keras.WandbModelCheckpoint(\n",
    "        #     str(CFG.ROOT / 'models' / CFG.MODEL / f\"{save_time}.h5\"),\n",
    "        #     monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        #     save_weights_only=True, options=options,\n",
    "        # )\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gCIJLIgW7tqS"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44793,
     "status": "ok",
     "timestamp": 1697362462918,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "rPcaVRUTNo76",
    "outputId": "ebaac3a3-53dc-4130-fee7-b4be9cce5a03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:27:41.132542: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-26 12:27:41.177060: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-10-26 12:27:41.448547: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:27:41.599988: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    img_adjust_layer = tf.keras.layers.Lambda(\n",
    "        lambda data: tf.keras.applications.imagenet_utils.preprocess_input(\n",
    "            tf.cast(data, tf.float32), mode=\"torch\"),\n",
    "        input_shape=[*CFG.IMAGE_SIZE, 3])\n",
    "    pretrained_model = tf.keras.models.load_model(\n",
    "        CFG.ROOT / 'base_models' / CFG.MODEL / 'base_model', compile=False\n",
    "    )\n",
    "    model = tf.keras.Sequential([\n",
    "        img_adjust_layer,\n",
    "        pretrained_model,\n",
    "        tf.keras.layers.Dense(len(class_dict), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    top3_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(\n",
    "        k=3, name='sparse_top_3_categorical_accuracy'\n",
    "    )\n",
    "    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=CFG.LR_START,\n",
    "        decay_steps=CFG.DECAY_STEPS\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(lr_decayed_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1697362463757,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "p3_FQHh3I6Dh",
    "outputId": "6a905fd1-fc10-4b2a-a659-9d0830599037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " swin_large_224 (SwinTransf  (None, 1536)              195331616 \n",
      " ormerModel)                                                     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 467)               717779    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196049395 (749.15 MB)\n",
      "Trainable params: 195713255 (746.59 MB)\n",
      "Non-trainable params: 336140 (2.56 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,  # lion.Lion(learning_rate=lr_decayed_fn),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy', top3_acc],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1697362464243,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "aQal3tMTJrFp"
   },
   "outputs": [],
   "source": [
    "# service_addr = tpu.get_master().replace(':8470', ':8466')\n",
    "# print(service_addr)\n",
    "# %tensorboard --logdir={log_dir + \"/tf\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1697362464922,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "Mm2Kj0Y9_gtM"
   },
   "outputs": [],
   "source": [
    "wandb.config = CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1734283,
     "status": "ok",
     "timestamp": 1697364199201,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "fmM3Sf5ONo77",
    "outputId": "c7845e25-c185-40d6-cdfb-c1e52e7df9e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:29:32.891956: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:29:32.916138: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:29:32.939980: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:29:32.963671: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:29:32.987170: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:29:33.010724: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:29:33.036115: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:42.440784: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:241] Subgraph fingerprint:17203030566503772247\n",
      "2023-10-26 12:30:48.945044: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n",
      "2023-10-26 12:30:57.938627: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.941880: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.941979: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.942037: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.942128: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.942204: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.942306: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.942383: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.942531: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.942679: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:30:57.996106: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(6f27142f57f610b1:0:0), session_name()\n",
      "2023-10-26 12:33:13.485361: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:226] Compilation of 6f27142f57f610b1:0:0 with session name  took 2m15.489164993s and succeeded\n",
      "2023-10-26 12:33:13.982627: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(6f27142f57f610b1:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_train_function_17203030566503772247\", property.function_library_fingerprint = 13196011770277986556, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"8,224,224,3,;8,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2023-10-26 12:33:13.982698: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:541] After adding entry for key 6f27142f57f610b1:0:0 with session_name  cache is 1 entries (485981077 bytes),  marked for eviction 0 entries (0 bytes).\n",
      "2023-10-26 12:33:14.016811: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.016910: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.016982: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.017046: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.017125: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.017181: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.017284: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.017378: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/189 [..............................] - ETA: 11:33:48 - loss: 6.4501 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:33:14.549158: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.549582: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.552268: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.552781: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n",
      "2023-10-26 12:33:14.554408: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - ETA: 0s - loss: 3.4266 - sparse_categorical_accuracy: 0.2906 - sparse_top_3_categorical_accuracy: 0.4623"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:34:19.294207: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:241] Subgraph fingerprint:5501623392189956025\n",
      "2023-10-26 12:34:20.487725: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2023-10-26 12:34:22.740829: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:441] TPU host compilation cache miss: cache_key(dd5da7931fbbd6c7:0:0), session_name()\n",
      "2023-10-26 12:34:52.836526: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:226] Compilation of dd5da7931fbbd6c7:0:0 with session name  took 30.095612791s and succeeded\n",
      "2023-10-26 12:34:52.933905: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:475] TPU host compilation cache: compilation complete for cache_key(dd5da7931fbbd6c7:0:0), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_test_function_5501623392189956025\", property.function_library_fingerprint = 2710679947185007848, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"8,224,224,3,;8,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2023-10-26 12:34:52.933982: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:541] After adding entry for key dd5da7931fbbd6c7:0:0 with session_name  cache is 2 entries (609355001 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:34:53.901791: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 321s 530ms/step - loss: 3.4266 - sparse_categorical_accuracy: 0.2906 - sparse_top_3_categorical_accuracy: 0.4623 - val_loss: 2.4894 - val_sparse_categorical_accuracy: 0.4355 - val_sparse_top_3_categorical_accuracy: 0.6367\n",
      "Epoch 2/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.8074 - sparse_categorical_accuracy: 0.5360 - sparse_top_3_categorical_accuracy: 0.7570WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:35:57.293683: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 63s 335ms/step - loss: 1.8074 - sparse_categorical_accuracy: 0.5360 - sparse_top_3_categorical_accuracy: 0.7570 - val_loss: 1.8633 - val_sparse_categorical_accuracy: 0.5410 - val_sparse_top_3_categorical_accuracy: 0.7539\n",
      "Epoch 3/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.3117 - sparse_categorical_accuracy: 0.6451 - sparse_top_3_categorical_accuracy: 0.8433WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:37:00.968318: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 64s 337ms/step - loss: 1.3117 - sparse_categorical_accuracy: 0.6451 - sparse_top_3_categorical_accuracy: 0.8433 - val_loss: 1.3961 - val_sparse_categorical_accuracy: 0.6562 - val_sparse_top_3_categorical_accuracy: 0.8242\n",
      "Epoch 4/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.0235 - sparse_categorical_accuracy: 0.7210 - sparse_top_3_categorical_accuracy: 0.8895WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:38:04.849960: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 64s 338ms/step - loss: 1.0235 - sparse_categorical_accuracy: 0.7210 - sparse_top_3_categorical_accuracy: 0.8895 - val_loss: 1.0841 - val_sparse_categorical_accuracy: 0.7227 - val_sparse_top_3_categorical_accuracy: 0.8750\n",
      "Epoch 5/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8395 - sparse_categorical_accuracy: 0.7690 - sparse_top_3_categorical_accuracy: 0.9129WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:39:08.704818: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 64s 338ms/step - loss: 0.8395 - sparse_categorical_accuracy: 0.7690 - sparse_top_3_categorical_accuracy: 0.9129 - val_loss: 1.0088 - val_sparse_categorical_accuracy: 0.7285 - val_sparse_top_3_categorical_accuracy: 0.8867\n",
      "Epoch 6/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7769 - sparse_categorical_accuracy: 0.7793 - sparse_top_3_categorical_accuracy: 0.9255WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:40:12.902471: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 64s 339ms/step - loss: 0.7769 - sparse_categorical_accuracy: 0.7793 - sparse_top_3_categorical_accuracy: 0.9255 - val_loss: 0.8333 - val_sparse_categorical_accuracy: 0.7656 - val_sparse_top_3_categorical_accuracy: 0.9219\n",
      "Epoch 7/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7687 - sparse_categorical_accuracy: 0.7838 - sparse_top_3_categorical_accuracy: 0.9253WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:41:17.249822: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 64s 340ms/step - loss: 0.7687 - sparse_categorical_accuracy: 0.7838 - sparse_top_3_categorical_accuracy: 0.9253 - val_loss: 0.9663 - val_sparse_categorical_accuracy: 0.7500 - val_sparse_top_3_categorical_accuracy: 0.8965\n",
      "Epoch 8/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7686 - sparse_categorical_accuracy: 0.7854 - sparse_top_3_categorical_accuracy: 0.9253WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:42:21.272249: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 64s 339ms/step - loss: 0.7686 - sparse_categorical_accuracy: 0.7854 - sparse_top_3_categorical_accuracy: 0.9253 - val_loss: 1.0039 - val_sparse_categorical_accuracy: 0.7324 - val_sparse_top_3_categorical_accuracy: 0.8711\n",
      "Epoch 9/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.5560 - sparse_categorical_accuracy: 0.8373 - sparse_top_3_categorical_accuracy: 0.9571WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:43:24.455119: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 63s 334ms/step - loss: 0.5560 - sparse_categorical_accuracy: 0.8373 - sparse_top_3_categorical_accuracy: 0.9571 - val_loss: 0.9130 - val_sparse_categorical_accuracy: 0.7637 - val_sparse_top_3_categorical_accuracy: 0.8887\n",
      "Epoch 10/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.3649 - sparse_categorical_accuracy: 0.8981 - sparse_top_3_categorical_accuracy: 0.9796WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:44:26.462541: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 62s 328ms/step - loss: 0.3649 - sparse_categorical_accuracy: 0.8981 - sparse_top_3_categorical_accuracy: 0.9796 - val_loss: 1.0061 - val_sparse_categorical_accuracy: 0.7305 - val_sparse_top_3_categorical_accuracy: 0.8848\n",
      "Epoch 11/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.2896 - sparse_categorical_accuracy: 0.9229 - sparse_top_3_categorical_accuracy: 0.9854WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:45:30.072373: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 64s 336ms/step - loss: 0.2896 - sparse_categorical_accuracy: 0.9229 - sparse_top_3_categorical_accuracy: 0.9854 - val_loss: 1.0339 - val_sparse_categorical_accuracy: 0.7266 - val_sparse_top_3_categorical_accuracy: 0.8809\n",
      "Epoch 12/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.3194 - sparse_categorical_accuracy: 0.9200 - sparse_top_3_categorical_accuracy: 0.9838WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:46:31.776233: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 62s 326ms/step - loss: 0.3194 - sparse_categorical_accuracy: 0.9200 - sparse_top_3_categorical_accuracy: 0.9838 - val_loss: 1.0165 - val_sparse_categorical_accuracy: 0.7363 - val_sparse_top_3_categorical_accuracy: 0.8828\n",
      "Epoch 13/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.5805 - sparse_categorical_accuracy: 0.8388 - sparse_top_3_categorical_accuracy: 0.9501WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:47:33.015736: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 61s 324ms/step - loss: 0.5805 - sparse_categorical_accuracy: 0.8388 - sparse_top_3_categorical_accuracy: 0.9501 - val_loss: 0.7957 - val_sparse_categorical_accuracy: 0.7754 - val_sparse_top_3_categorical_accuracy: 0.9199\n",
      "Epoch 14/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7699 - sparse_categorical_accuracy: 0.7804 - sparse_top_3_categorical_accuracy: 0.9274WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:48:34.924897: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 62s 327ms/step - loss: 0.7699 - sparse_categorical_accuracy: 0.7804 - sparse_top_3_categorical_accuracy: 0.9274 - val_loss: 1.0410 - val_sparse_categorical_accuracy: 0.7266 - val_sparse_top_3_categorical_accuracy: 0.8828\n",
      "Epoch 15/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7697 - sparse_categorical_accuracy: 0.7830 - sparse_top_3_categorical_accuracy: 0.9254WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:49:36.214447: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 61s 324ms/step - loss: 0.7697 - sparse_categorical_accuracy: 0.7830 - sparse_top_3_categorical_accuracy: 0.9254 - val_loss: 1.0099 - val_sparse_categorical_accuracy: 0.7246 - val_sparse_top_3_categorical_accuracy: 0.8848\n",
      "Epoch 16/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7701 - sparse_categorical_accuracy: 0.7815 - sparse_top_3_categorical_accuracy: 0.9244WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:50:37.469292: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 61s 324ms/step - loss: 0.7701 - sparse_categorical_accuracy: 0.7815 - sparse_top_3_categorical_accuracy: 0.9244 - val_loss: 1.0309 - val_sparse_categorical_accuracy: 0.7383 - val_sparse_top_3_categorical_accuracy: 0.8828\n",
      "Epoch 17/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.5644 - sparse_categorical_accuracy: 0.8390 - sparse_top_3_categorical_accuracy: 0.9540WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:51:39.256816: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 62s 327ms/step - loss: 0.5644 - sparse_categorical_accuracy: 0.8390 - sparse_top_3_categorical_accuracy: 0.9540 - val_loss: 0.9764 - val_sparse_categorical_accuracy: 0.7441 - val_sparse_top_3_categorical_accuracy: 0.8887\n",
      "Epoch 18/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.3755 - sparse_categorical_accuracy: 0.8929 - sparse_top_3_categorical_accuracy: 0.9773WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:52:41.209428: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 62s 327ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8929 - sparse_top_3_categorical_accuracy: 0.9773 - val_loss: 0.8220 - val_sparse_categorical_accuracy: 0.7617 - val_sparse_top_3_categorical_accuracy: 0.9160\n",
      "Epoch 19/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.2846 - sparse_categorical_accuracy: 0.9225 - sparse_top_3_categorical_accuracy: 0.9878WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:53:43.460830: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 62s 329ms/step - loss: 0.2846 - sparse_categorical_accuracy: 0.9225 - sparse_top_3_categorical_accuracy: 0.9878 - val_loss: 0.9960 - val_sparse_categorical_accuracy: 0.7402 - val_sparse_top_3_categorical_accuracy: 0.8730\n",
      "Epoch 20/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.3209 - sparse_categorical_accuracy: 0.9206 - sparse_top_3_categorical_accuracy: 0.9830WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:54:44.654317: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_sparse_categorical_crossentropy` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_3_categorical_accuracy,val_loss,val_sparse_categorical_accuracy,val_sparse_top_3_categorical_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 61s 323ms/step - loss: 0.3209 - sparse_categorical_accuracy: 0.9206 - sparse_top_3_categorical_accuracy: 0.9830 - val_loss: 0.9886 - val_sparse_categorical_accuracy: 0.7461 - val_sparse_top_3_categorical_accuracy: 0.8867\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    get_training_dataset(),\n",
    "    steps_per_epoch=CFG.STEPS_PER_EPOCH,\n",
    "    epochs=CFG.EPOCHS,\n",
    "    validation_data=get_validation_dataset(),\n",
    "    validation_steps=CFG.VALIDATION_STEPS,\n",
    "    callbacks=make_callbacks(CFG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of jupyter_server.serverapp failed: Traceback (most recent call last):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/jupyter_server/serverapp.py\", line 77, in <module>\n",
      "    from jupyter_server.auth.identity import (\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/jupyter_server/auth/identity.py\", line 28, in <module>\n",
      "    from .utils import get_anonymous_username\n",
      "ImportError: cannot import name 'get_anonymous_username' from 'jupyter_server.auth.utils' (/home/broug/.local/lib/python3.10/site-packages/jupyter_server/auth/utils.py)\n",
      "]\n",
      "[autoreload of jupyter_server.base.handlers failed: Traceback (most recent call last):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: get_current_user() requires a code object with 1 free vars, not 0\n",
      "]\n",
      "[autoreload of jupyter_server.auth.authorizer failed: Traceback (most recent call last):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/jupyter_server/auth/authorizer.py\", line 17, in <module>\n",
      "    from .identity import IdentityProvider, User\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/jupyter_server/auth/identity.py\", line 28, in <module>\n",
      "    from .utils import get_anonymous_username\n",
      "ImportError: cannot import name 'get_anonymous_username' from 'jupyter_server.auth.utils' (/home/broug/.local/lib/python3.10/site-packages/jupyter_server/auth/utils.py)\n",
      "]\n",
      "[autoreload of jupyter_server.prometheus.metrics failed: Traceback (most recent call last):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/jupyter_server/prometheus/metrics.py\", line 12, in <module>\n",
      "    from notebook.prometheus.metrics import (  # type:ignore[import-not-found]\n",
      "ModuleNotFoundError: No module named 'notebook'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/jupyter_server/prometheus/metrics.py\", line 21, in <module>\n",
      "    HTTP_REQUEST_DURATION_SECONDS = Histogram(\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/prometheus_client/metrics.py\", line 558, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/prometheus_client/metrics.py\", line 143, in __init__\n",
      "    registry.register(self)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/prometheus_client/registry.py\", line 43, in register\n",
      "    raise ValueError(\n",
      "ValueError: Duplicated timeseries in CollectorRegistry: {'http_request_duration_seconds_bucket', 'http_request_duration_seconds_count', 'http_request_duration_seconds_sum', 'http_request_duration_seconds', 'http_request_duration_seconds_created'}\n",
      "]\n",
      "[autoreload of jupyter_server.gateway.managers failed: Traceback (most recent call last):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/broug/.local/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: start_kernel() requires a code object with 0 free vars, not 2\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQzOguVCg16L"
   },
   "outputs": [],
   "source": [
    "art = wandb.Artifact(\n",
    "    'model',\n",
    "    type='model')\n",
    "art.add_file(str(CFG.ROOT / 'models' / CFG.MODEL / f\"{time}.h5\"))\n",
    "wandb.log_artifact(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 1578,
     "status": "ok",
     "timestamp": 1697364201212,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "qfCXrWuFNo77",
    "outputId": "7826babb-7c86-448c-a098-bee428d54fa9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0b107be7bb99>:92: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
      "  ax = plt.subplot(subplot)\n",
      "/usr/local/lib/python3.10/dist-packages/plotly/matplotlylib/renderer.py:647: UserWarning:\n",
      "\n",
      "Looks like the annotation(s) you are trying \n",
      "to draw lies/lay outside the given figure size.\n",
      "\n",
      "Therefore, the resulting Plotly figure may not be \n",
      "large enough to view the full text. To adjust \n",
      "the size of the figure, use the 'width' and \n",
      "'height' keys in the Layout object. Alternatively,\n",
      "use the Margin object to adjust the figure's margins.\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n",
      "\n",
      "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n",
    "display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7a95e6bcfca34c5ca67d863895380872",
      "88960c221ba443f5b10a8a63dc773217",
      "e249c6d64080430ca273dacb2cae446f",
      "b707fc265eb1412ebb8d455d32b609d0",
      "d397f7b7138e49a8a6801d96e7a8549b",
      "dc5a275727a745b9851b68716f123861",
      "ccb468c6897448c5ad68ea166f78f027",
      "1c3cc56bb26b46bdab17e364d28c66ed"
     ]
    },
    "executionInfo": {
     "elapsed": 6494,
     "status": "ok",
     "timestamp": 1697364207703,
     "user": {
      "displayName": "graham broughton",
      "userId": "15728648374086258761"
     },
     "user_tz": 420
    },
    "id": "EP-fMeipNfKG",
    "outputId": "b91b04b5-c590-4025-8cee-f42d5f13a8d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">skilled-leaf-80</strong> at: <a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/1gj7898x' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier/runs/1gj7898x</a><br/> View job at <a href='https://wandb.ai/g-broughton/Mushroom-Classifier/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDExMDI4Mw==/version_details/v2' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDExMDI4Mw==/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_122647-1gj7898x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpdijuMKjUo7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1iUOXvvXCq1WNVkG_PUA9YWR5SgaNB6ww",
     "timestamp": 1697270080331
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c3cc56bb26b46bdab17e364d28c66ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a95e6bcfca34c5ca67d863895380872": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_88960c221ba443f5b10a8a63dc773217",
       "IPY_MODEL_e249c6d64080430ca273dacb2cae446f"
      ],
      "layout": "IPY_MODEL_b707fc265eb1412ebb8d455d32b609d0"
     }
    },
    "88960c221ba443f5b10a8a63dc773217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d397f7b7138e49a8a6801d96e7a8549b",
      "placeholder": "",
      "style": "IPY_MODEL_dc5a275727a745b9851b68716f123861",
      "value": "4498.752 MB of 4498.795 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "b707fc265eb1412ebb8d455d32b609d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccb468c6897448c5ad68ea166f78f027": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d397f7b7138e49a8a6801d96e7a8549b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc5a275727a745b9851b68716f123861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e249c6d64080430ca273dacb2cae446f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccb468c6897448c5ad68ea166f78f027",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c3cc56bb26b46bdab17e364d28c66ed",
      "value": 0.9999903538631951
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
