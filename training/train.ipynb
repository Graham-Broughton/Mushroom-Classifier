{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["!pip install -q tensorflow==2.10.0 wandb python-dotenv tensorboard_plugin_profile tensorflow_io==0.27.0"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":553,"status":"ok","timestamp":1697361847933,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"PKiDhRvyNwiw","outputId":"166b32ab-cf89-4855-854e-283c05ec1158"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/Mushroom-Classifier'\n","/home/broug/Desktop/Mushroom-Classifier/training\n"]}],"source":["from pathlib import Path\n","import os\n","\n","try:\n","    import wandb\n","except:\n","    if os.environ['COLAB_RELEASE_TAG']:\n","        print(\"Found Colab Environment\")\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        from google.colab import auth\n","        auth.authenticate_user()\n","\n","        %pip install -q tensorflow==2.10.0 wandb python-dotenv tensorboard_plugin_profile tensorflow_io==0.27.0\n","        exit()\n","    elif Path().cwd().name == 'Mushroom-Classifier':\n","        print(\"Found Other Environment\")\n","        %pip install -q tensorflow==2.10.0 wandb python-dotenv tensorboard_plugin_profile tensorflow_io==0.27.0\n","        exit()\n","    else:\n","        print('Please run this notebook from the root of the repository')\n","        exit()\n","\n","%cd /content/drive/MyDrive/Mushroom-Classifier"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1697361848140,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"GEQKIS-h_JS2"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3230,"status":"ok","timestamp":1697361852221,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"LfAu_aLnNo7l","outputId":"9eb39522-b6f7-46bf-e0e8-74635f093a9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensorflow version 2.10.0\n"]}],"source":["import math, re, os, pickle\n","import tensorflow as tf\n","from datetime import datetime\n","import wandb\n","from wandb.keras import WandbCallback, WandbModelCheckpoint\n","import numpy as np\n","from matplotlib import pyplot as plt\n","# from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n","from src.models.swintransformer import SwinTransformer\n","# from src.optimizers import lion\n","# from prefect import task, flow\n","\n","print(f\"Tensorflow version {tf.__version__}\")\n","AUTO = tf.data.experimental.AUTOTUNE\n","np.set_printoptions(threshold=15, linewidth=80)\n","\n","from config import GCFG, CFG\n","\n","CFG2 = GCFG()"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"elapsed":10757,"status":"ok","timestamp":1697361887542,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"_-bkudXcZiWd","outputId":"1c194518-0412-4424-8e1b-f358bd232512"},"outputs":[],"source":["save_time = datetime.now().strftime('%m%d-%H%M')\n","log_dir = f\"{CFG2.GCS_REPO}/logs/{CFG2.MODEL}/{save_time}\"\n","\n","# wandb.tensorboard.patch(root_logdir=log_dir + \"/tf\")\n","# wandb.init(project=\"Mushroom-Classifier\", tags=[f'{CFG2.MODEL}', \"Adam - Cosine\", str(CFG2.IMAGE_SIZE[0])])"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30289,"status":"ok","timestamp":1697361917827,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"PBI-WAppNo7s","outputId":"955d1b29-9dfb-45aa-dc46-9cbd9c462c7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of accelerators:  1\n"]}],"source":["# Detect hardware\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","except ValueError:  # If TPU not found\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy()\n","CFG2.REPLICAS = strategy.num_replicas_in_sync\n","print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"]},{"cell_type":"markdown","metadata":{"id":"nnHHCJk-No7v"},"source":["## Visualization Utils"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":563,"status":"ok","timestamp":1697361918383,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"yTKGP-V2No7x"},"outputs":[],"source":["def batch_to_numpy_images_and_labels(data):\n","    images, labels = data\n","    numpy_images = images.numpy()\n","    numpy_labels = labels.numpy()\n","    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n","        numpy_labels = [None for _ in enumerate(numpy_images)]\n","    # If no labels, only image IDs, return None for labels (this is the case for test data)\n","    return numpy_images, numpy_labels\n","\n","def title_from_label_and_target(label, correct_label):\n","    if correct_label is None:\n","        return class_dict[label], True\n","    correct = (label == correct_label)\n","    return \"{} [{}{}{}]\".format(class_dict[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n","                                class_dict[correct_label] if not correct else ''), correct\n","\n","def display_one_flower(image, title, subplot, red=False, titlesize=16):\n","    plt.subplot(*subplot)\n","    plt.axis('off')\n","    plt.imshow(image)\n","    if len(title) > 0:\n","        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n","    return (subplot[0], subplot[1], subplot[2]+1)\n","\n","def display_batch_of_images(databatch, predictions=None):\n","    \"\"\"This will work with:\n","    display_batch_of_images(images)\n","    display_batch_of_images(images, predictions)\n","    display_batch_of_images((images, labels))\n","    display_batch_of_images((images, labels), predictions)\n","    \"\"\"\n","    # data\n","    images, labels = batch_to_numpy_images_and_labels(databatch)\n","    if labels is None:\n","        labels = [None for _ in enumerate(images)]\n","\n","    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n","    rows = int(math.sqrt(len(images)))\n","    cols = len(images)//rows\n","\n","    # size and spacing\n","    FIGSIZE = 13.0\n","    SPACING = 0.1\n","    subplot=(rows,cols,1)\n","    if rows < cols:\n","        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n","    else:\n","        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n","\n","    # display\n","    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n","        title = '' if label is None else class_dict[label]\n","        correct = True\n","        if predictions is not None:\n","            title, correct = title_from_label_and_target(predictions[i], label)\n","        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n","        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n","\n","    #layout\n","    plt.tight_layout()\n","    if label is None and predictions is None:\n","        plt.subplots_adjust(wspace=0, hspace=0)\n","    else:\n","        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n","    plt.show()\n","\n","def display_confusion_matrix(cmat, score, precision, recall):\n","    plt.figure(figsize=(15,15))\n","    ax = plt.gca()\n","    ax.matshow(cmat, cmap='Reds')\n","    ax.set_xticks(range(len(class_dict)))\n","    ax.set_xticklabels(class_dict, fontdict={'fontsize': 7})\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n","    ax.set_yticks(range(len(class_dict)))\n","    ax.set_yticklabels(class_dict, fontdict={'fontsize': 7})\n","    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","    titlestring = \"\"\n","    if score is not None:\n","        titlestring += 'f1 = {:.3f} '.format(score)\n","    if precision is not None:\n","        titlestring += '\\nprecision = {:.3f} '.format(precision)\n","    if recall is not None:\n","        titlestring += '\\nrecall = {:.3f} '.format(recall)\n","    if titlestring != \"\":\n","        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n","    plt.show()\n","\n","def display_training_curves(training, validation, title, subplot):\n","    if subplot%10==1: # set up the subplots on the first call\n","        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n","        plt.tight_layout()\n","    ax = plt.subplot(subplot)\n","    ax.set_facecolor('#F8F8F8')\n","    ax.plot(training)\n","    ax.plot(validation)\n","    ax.set_title(f'model {title}')\n","    ax.set_ylabel(title)\n","    #ax.set_ylim(0.28,1.05)\n","    ax.set_xlabel('epoch')\n","    ax.legend(['train', 'valid.'])\n","    wandb.log({\"chart\": plt})\n","    path = CFG.ROOT / \"images\" / CFG.MODEL\n","    path.mkdir(exist_ok=True)\n","    plt.savefig(path / f'{title}-{save_time}.png')"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":683,"status":"ok","timestamp":1697361919060,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"8ynfISf3No7z"},"outputs":[],"source":["def decode_image(image_data):\n","    image = tf.image.decode_jpeg(image_data, channels=3)  # image format uint8 [0,255]\n","    image = tf.reshape(image, [*CFG.IMAGE_SIZE, 3]) # explicit size needed for TPU\n","    return image\n","\n","\n","def read_labeled_tfrecord(example):\n","    feature_description = {\n","        'image': tf.io.FixedLenFeature([], tf.string),\n","        'dataset': tf.io.FixedLenFeature([], tf.int64),\n","        'longitude': tf.io.FixedLenFeature([], tf.float32),\n","        'latitude': tf.io.FixedLenFeature([], tf.float32),\n","        'norm_date': tf.io.FixedLenFeature([], tf.float32),\n","        'class_priors': tf.io.FixedLenFeature([], tf.float32),\n","        'class_id': tf.io.FixedLenFeature([], tf.int64),\n","    }\n","    example = tf.io.parse_single_example(example, feature_description)\n","    image = decode_image(example['image'])\n","    label = tf.cast(example['class_id'], tf.int32)\n","    return image, label\n","\n","\n","def load_dataset(filenames, labeled=True, ordered=False):\n","    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n","    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n","\n","    ignore_order = tf.data.Options()\n","    if not ordered:\n","        ignore_order.experimental_deterministic = False # disable order, increase speed\n","\n","    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n","    dataset = dataset.cache()\n","    dataset = dataset.shuffle(CFG.BATCH_SIZE * 10)\n","    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n","    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO) # if labeled else read_unlabeled_tfrecord\n","    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n","    return dataset\n","\n","def data_augment(image, label):\n","    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n","    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n","    # of the TPU while the TPU itself is computing gradients.\n","    # image = tf.image.random_flip_left_right(image)\n","    #image = tf.image.random_saturation(image, 0, 2)\n","    return image, label\n","\n","def get_training_dataset():\n","    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n","    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n","     # the training dataset must repeat for several epochs\n","    dataset = dataset.batch(CFG.BATCH_SIZE)\n","    dataset = dataset.repeat()\n","    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def get_validation_dataset(ordered=False):\n","    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n","    dataset = dataset.batch(CFG.BATCH_SIZE)\n","    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n","    return dataset\n","\n","def count_data_items(filenames):\n","    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n","    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n","    return np.sum(n)"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":762,"status":"ok","timestamp":1697361919818,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"_T0iLwTwNo71"},"outputs":[],"source":["GCS_PATH_SELECT = {\n","    192: f'{CFG2.GCS_REPO}/tfrecords-jpeg-192x192',\n","    224: f'{CFG2.GCS_REPO}/tfrecords-jpeg-224x224v2',\n","    384: f'{CFG2.GCS_REPO}/tfrecords-jpeg-384x384',\n","    512: f'{CFG2.GCS_REPO}/tfrecords-jpeg-512x512',\n","}\n","GCS_PATH = GCS_PATH_SELECT[CFG2.IMAGE_SIZE[0]]\n","TRAINING_FILENAMES = tf.io.gfile.glob(f'{GCS_PATH}/train*.tfrec')\n","VALIDATION_FILENAMES = tf.io.gfile.glob(f'{GCS_PATH}/val*.tfrec')\n","\n","class_dict = pickle.load(open('src/class_dict.pkl', 'rb'))\n","\n","CFG2.NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n","CFG2.NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n","\n","CFG = CFG(REPLICAS=CFG2.REPLICAS, NUM_TRAINING_IMAGES=CFG2.NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES=CFG2.NUM_VALIDATION_IMAGES)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"cC5l84OONo73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data shapes:\n"]},{"name":"stderr","output_type":"stream","text":["2023-10-24 03:22:59.032971: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2023-10-24 03:22:59.033006: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n","2023-10-24 03:22:59.033034: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-3): /proc/driver/nvidia/version does not exist\n","2023-10-24 03:22:59.034187: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"name":"stdout","output_type":"stream","text":["(8, 224, 224, 3) (8,)\n","(8, 224, 224, 3) (8,)\n","(8, 224, 224, 3) (8,)\n"]},{"name":"stderr","output_type":"stream","text":["2023-10-24 03:22:59.704887: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"]},{"name":"stdout","output_type":"stream","text":["Training data label examples: [135 296  41 463  63 184  18 378]\n","Validation data shapes:\n","(8, 224, 224, 3) (8,)\n","(8, 224, 224, 3) (8,)\n","(8, 224, 224, 3) (8,)\n"]},{"name":"stderr","output_type":"stream","text":["2023-10-24 03:23:00.476477: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"]},{"name":"stdout","output_type":"stream","text":["Validation data label examples: [277 218 239   7 226 373 415 434]\n"]}],"source":["# data dump\n","print(\"Training data shapes:\")\n","for image, label in get_training_dataset().take(3):\n","    print(image.numpy().shape, label.numpy().shape)\n","print(\"Training data label examples:\", label.numpy())\n","print(\"Validation data shapes:\")\n","for image, label in get_validation_dataset().take(3):\n","    print(image.numpy().shape, label.numpy().shape)\n","print(\"Validation data label examples:\", label.numpy())"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SYJ_luSVNo74"},"outputs":[],"source":["# Peek at training data\n","training_dataset = get_training_dataset()\n","training_dataset = training_dataset.unbatch().batch(20)\n","train_batch = iter(training_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvzhBMQ_No75"},"outputs":[],"source":["# run this cell again for next set of images\n","# display_batch_of_images(next(train_batch))"]},{"cell_type":"markdown","metadata":{"id":"K1_Bez6dNo76"},"source":["you can select from these models:\n","- swin_tiny_224\n","- swin_small_224\n","- swin_base_224\n","- swin_base_384\n","- swin_large_224\n","- swin_large_384"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":735,"status":"ok","timestamp":1697362294835,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"6V4wIMBH5xPK"},"outputs":[],"source":["def make_callbacks(CFG):\n","    # options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n","    options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n","\n","    callbacks = [\n","        # tf.keras.callbacks.EarlyStopping(\n","        #     monitor=\"val_loss\",\n","        #     patience=CFG.ES_PATIENCE,\n","        #     verbose=1,\n","        #     restore_best_weights=True,\n","        # ),\n","        # tf.keras.callbacks.TensorBoard(log_dir=log_dir + \"/tf\", profile_batch=(50, 250)),\n","        tf.keras.callbacks.CSVLogger(\n","            filename=f'{CFG.GCS_REPO}/logs/{save_time}-csv_log.csv',\n","            separator=\",\",\n","            append=False,\n","        ),\n","        # wandb.keras.WandbMetricsLogger(log_freq='batch'),\n","        # wandb.keras.WandbModelCheckpoint(\n","        #     str(CFG.ROOT / 'models' / CFG.MODEL / f\"{save_time}.h5\"),\n","        #     monitor='val_loss', verbose=1, save_best_only=True,\n","        #     save_weights_only=True, options=options,\n","        # )\n","    ]\n","    return callbacks"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"gCIJLIgW7tqS"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44793,"status":"ok","timestamp":1697362462918,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"rPcaVRUTNo76","outputId":"ebaac3a3-53dc-4130-fee7-b4be9cce5a03"},"outputs":[],"source":["with strategy.scope():\n","    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode=\"torch\"), input_shape=[*CFG.IMAGE_SIZE, 3])\n","    pretrained_model = SwinTransformer(CFG.MODEL, num_classes=len(class_dict), include_top=False, pretrained=False, use_tpu=True)\n","    pretrained_model = tf.keras.Sequential([\n","        img_adjust_layer,\n","        pretrained_model,\n","        tf.keras.layers.Dense(len(class_dict), activation='softmax')\n","    ])\n","\n","    top3_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(\n","        k=3, name='sparse_top_3_categorical_accuracy'\n","    )\n","    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate=CFG.LR_START,\n","        decay_steps=CFG.DECAY_STEPS\n","    )\n","    def get_lr_metric(optimizer):\n","        def lr(y_true, y_pred):\n","            return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n","        return lr\n","\n","    optimizer = tf.keras.optimizers.Adam(lr_decayed_fn)\n","    lr_metric = get_lr_metric(optimizer)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["pretrained_model.load_weights(CFG.ROOT / 'base_models' / CFG.MODEL / 'base_model.h5')"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":845,"status":"ok","timestamp":1697362463757,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"p3_FQHh3I6Dh","outputId":"6a905fd1-fc10-4b2a-a659-9d0830599037"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_29\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lambda_4 (Lambda)           (None, 384, 384, 3)       0         \n","                                                                 \n"," swin_base_384 (SwinTransfor  (None, 1024)             89781624  \n"," merModel)                                                       \n","                                                                 \n"," dense_4 (Dense)             (None, 467)               478675    \n","                                                                 \n","=================================================================\n","Total params: 90,260,299\n","Trainable params: 87,357,259\n","Non-trainable params: 2,903,040\n","_________________________________________________________________\n"]}],"source":["pretrained_model.compile(\n","    optimizer= optimizer,  # lion.Lion(learning_rate=lr_decayed_fn),\n","    loss = 'sparse_categorical_crossentropy',\n","    metrics=['sparse_categorical_accuracy', lr_metric, top3_acc],\n",")\n","pretrained_model.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":505,"status":"ok","timestamp":1697362464243,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"aQal3tMTJrFp"},"outputs":[],"source":["# service_addr = tpu.get_master().replace(':8470', ':8466')\n","# print(service_addr)\n","# %tensorboard --logdir={log_dir + \"/tf\"}"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":683,"status":"ok","timestamp":1697362464922,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"Mm2Kj0Y9_gtM"},"outputs":[],"source":["wandb.config = CFG"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1734283,"status":"ok","timestamp":1697364199201,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"fmM3Sf5ONo77","outputId":"c7845e25-c185-40d6-cdfb-c1e52e7df9e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","  6/189 [..............................] - ETA: 1:05 - loss: 6.2224 - sparse_categorical_accuracy: 0.0078 - lr: 3.9999e-04 - sparse_top_3_categorical_accuracy: 0.0156"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 15.4779s). Check your callbacks.\n"]},{"name":"stdout","output_type":"stream","text":["189/189 [==============================] - ETA: 0s - loss: 5.9873 - sparse_categorical_accuracy: 0.0052 - lr: 3.8836e-04 - sparse_top_3_categorical_accuracy: 0.0130\n","Epoch 1: val_loss improved from inf to 6.09191, saving model to /content/drive/MyDrive/Mushroom-Classifier/models/swin_large_224/1015-0924.h5\n","189/189 [==============================] - 261s 560ms/step - loss: 5.9873 - sparse_categorical_accuracy: 0.0052 - lr: 3.8836e-04 - sparse_top_3_categorical_accuracy: 0.0130 - val_loss: 6.0919 - val_sparse_categorical_accuracy: 0.0078 - val_lr: 3.6577e-04 - val_sparse_top_3_categorical_accuracy: 0.0098\n","Epoch 2/20\n","189/189 [==============================] - ETA: 0s - loss: 3.8584 - sparse_categorical_accuracy: 0.2236 - lr: 3.2370e-04 - sparse_top_3_categorical_accuracy: 0.3695\n","Epoch 2: val_loss improved from 6.09191 to 2.80786, saving model to /content/drive/MyDrive/Mushroom-Classifier/models/swin_large_224/1015-0924.h5\n","189/189 [==============================] - 85s 450ms/step - loss: 3.8584 - sparse_categorical_accuracy: 0.2236 - lr: 3.2370e-04 - sparse_top_3_categorical_accuracy: 0.3695 - val_loss: 2.8079 - val_sparse_categorical_accuracy: 0.3574 - val_lr: 2.7479e-04 - val_sparse_top_3_categorical_accuracy: 0.6035\n","Epoch 3/20\n","189/189 [==============================] - ETA: 0s - loss: 1.7971 - sparse_categorical_accuracy: 0.5432 - lr: 2.1670e-04 - sparse_top_3_categorical_accuracy: 0.7541\n","Epoch 3: val_loss improved from 2.80786 to 1.76564, saving model to /content/drive/MyDrive/Mushroom-Classifier/models/swin_large_224/1015-0924.h5\n","189/189 [==============================] - 86s 457ms/step - loss: 1.7971 - sparse_categorical_accuracy: 0.5432 - lr: 2.1670e-04 - sparse_top_3_categorical_accuracy: 0.7541 - val_loss: 1.7656 - val_sparse_categorical_accuracy: 0.5410 - val_lr: 1.5821e-04 - val_sparse_top_3_categorical_accuracy: 0.7598\n","Epoch 4/20\n","189/189 [==============================] - ETA: 0s - loss: 1.2299 - sparse_categorical_accuracy: 0.6628 - lr: 1.0398e-04 - sparse_top_3_categorical_accuracy: 0.8553\n","Epoch 4: val_loss improved from 1.76564 to 1.21996, saving model to /content/drive/MyDrive/Mushroom-Classifier/models/swin_large_224/1015-0924.h5\n","189/189 [==============================] - 87s 462ms/step - loss: 1.2299 - sparse_categorical_accuracy: 0.6628 - lr: 1.0398e-04 - sparse_top_3_categorical_accuracy: 0.8553 - val_loss: 1.2200 - val_sparse_categorical_accuracy: 0.6758 - val_lr: 5.5938e-05 - val_sparse_top_3_categorical_accuracy: 0.8535\n","Epoch 5/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9954 - sparse_categorical_accuracy: 0.7260 - lr: 2.4125e-05 - sparse_top_3_categorical_accuracy: 0.8899\n","Epoch 5: val_loss improved from 1.21996 to 1.16929, saving model to /content/drive/MyDrive/Mushroom-Classifier/models/swin_large_224/1015-0924.h5\n","189/189 [==============================] - 87s 459ms/step - loss: 0.9954 - sparse_categorical_accuracy: 0.7260 - lr: 2.4125e-05 - sparse_top_3_categorical_accuracy: 0.8899 - val_loss: 1.1693 - val_sparse_categorical_accuracy: 0.6973 - val_lr: 2.9781e-06 - val_sparse_top_3_categorical_accuracy: 0.8652\n","Epoch 6/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9420 - sparse_categorical_accuracy: 0.7382 - lr: 2.8134e-07 - sparse_top_3_categorical_accuracy: 0.9002\n","Epoch 6: val_loss improved from 1.16929 to 1.06367, saving model to /content/drive/MyDrive/Mushroom-Classifier/models/swin_large_224/1015-0924.h5\n","189/189 [==============================] - 86s 454ms/step - loss: 0.9420 - sparse_categorical_accuracy: 0.7382 - lr: 2.8134e-07 - sparse_top_3_categorical_accuracy: 0.9002 - val_loss: 1.0637 - val_sparse_categorical_accuracy: 0.7109 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8789\n","Epoch 7/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9372 - sparse_categorical_accuracy: 0.7349 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.8996\n","Epoch 7: val_loss did not improve from 1.06367\n","189/189 [==============================] - 81s 426ms/step - loss: 0.9372 - sparse_categorical_accuracy: 0.7349 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.8996 - val_loss: 1.0870 - val_sparse_categorical_accuracy: 0.7070 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8848\n","Epoch 8/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9133 - sparse_categorical_accuracy: 0.7483 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9044\n","Epoch 8: val_loss did not improve from 1.06367\n","189/189 [==============================] - 73s 384ms/step - loss: 0.9133 - sparse_categorical_accuracy: 0.7483 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9044 - val_loss: 1.0919 - val_sparse_categorical_accuracy: 0.7051 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8828\n","Epoch 9/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9281 - sparse_categorical_accuracy: 0.7397 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9005\n","Epoch 9: val_loss did not improve from 1.06367\n","189/189 [==============================] - 73s 385ms/step - loss: 0.9281 - sparse_categorical_accuracy: 0.7397 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9005 - val_loss: 1.1636 - val_sparse_categorical_accuracy: 0.7012 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8691\n","Epoch 10/20\n","189/189 [==============================] - ETA: 0s - loss: 0.6504 - sparse_categorical_accuracy: 0.8128 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9435\n","Epoch 10: val_loss improved from 1.06367 to 1.03960, saving model to /content/drive/MyDrive/Mushroom-Classifier/models/swin_large_224/1015-0924.h5\n","189/189 [==============================] - 79s 418ms/step - loss: 0.6504 - sparse_categorical_accuracy: 0.8128 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9435 - val_loss: 1.0396 - val_sparse_categorical_accuracy: 0.7168 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8906\n","Epoch 11/20\n","189/189 [==============================] - ETA: 0s - loss: 0.3907 - sparse_categorical_accuracy: 0.8921 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9761\n","Epoch 11: val_loss did not improve from 1.03960\n","189/189 [==============================] - 77s 407ms/step - loss: 0.3907 - sparse_categorical_accuracy: 0.8921 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9761 - val_loss: 1.0975 - val_sparse_categorical_accuracy: 0.7031 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8828\n","Epoch 12/20\n","189/189 [==============================] - ETA: 0s - loss: 0.3754 - sparse_categorical_accuracy: 0.9054 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9783\n","Epoch 12: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 386ms/step - loss: 0.3754 - sparse_categorical_accuracy: 0.9054 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9783 - val_loss: 1.0893 - val_sparse_categorical_accuracy: 0.7070 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8809\n","Epoch 13/20\n","189/189 [==============================] - ETA: 0s - loss: 0.6747 - sparse_categorical_accuracy: 0.8228 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9384\n","Epoch 13: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 385ms/step - loss: 0.6747 - sparse_categorical_accuracy: 0.8228 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9384 - val_loss: 1.0885 - val_sparse_categorical_accuracy: 0.6953 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8770\n","Epoch 14/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9370 - sparse_categorical_accuracy: 0.7387 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9013\n","Epoch 14: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 385ms/step - loss: 0.9370 - sparse_categorical_accuracy: 0.7387 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9013 - val_loss: 1.1729 - val_sparse_categorical_accuracy: 0.6992 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8574\n","Epoch 15/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9404 - sparse_categorical_accuracy: 0.7359 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.8996\n","Epoch 15: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 384ms/step - loss: 0.9404 - sparse_categorical_accuracy: 0.7359 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.8996 - val_loss: 1.1283 - val_sparse_categorical_accuracy: 0.7012 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8789\n","Epoch 16/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9097 - sparse_categorical_accuracy: 0.7501 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9066\n","Epoch 16: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 384ms/step - loss: 0.9097 - sparse_categorical_accuracy: 0.7501 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9066 - val_loss: 1.1720 - val_sparse_categorical_accuracy: 0.6934 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8672\n","Epoch 17/20\n","189/189 [==============================] - ETA: 0s - loss: 0.9319 - sparse_categorical_accuracy: 0.7382 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9013\n","Epoch 17: val_loss did not improve from 1.03960\n","189/189 [==============================] - 74s 392ms/step - loss: 0.9319 - sparse_categorical_accuracy: 0.7382 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9013 - val_loss: 1.1570 - val_sparse_categorical_accuracy: 0.6934 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8652\n","Epoch 18/20\n","189/189 [==============================] - ETA: 0s - loss: 0.6597 - sparse_categorical_accuracy: 0.8116 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9419\n","Epoch 18: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 384ms/step - loss: 0.6597 - sparse_categorical_accuracy: 0.8116 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9419 - val_loss: 1.1325 - val_sparse_categorical_accuracy: 0.7129 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8633\n","Epoch 19/20\n","189/189 [==============================] - ETA: 0s - loss: 0.3901 - sparse_categorical_accuracy: 0.8942 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9758\n","Epoch 19: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 385ms/step - loss: 0.3901 - sparse_categorical_accuracy: 0.8942 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9758 - val_loss: 1.1005 - val_sparse_categorical_accuracy: 0.6836 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8789\n","Epoch 20/20\n","189/189 [==============================] - ETA: 0s - loss: 0.3781 - sparse_categorical_accuracy: 0.9043 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9772\n","Epoch 20: val_loss did not improve from 1.03960\n","189/189 [==============================] - 73s 385ms/step - loss: 0.3781 - sparse_categorical_accuracy: 0.9043 - lr: 0.0000e+00 - sparse_top_3_categorical_accuracy: 0.9772 - val_loss: 1.0844 - val_sparse_categorical_accuracy: 0.7227 - val_lr: 0.0000e+00 - val_sparse_top_3_categorical_accuracy: 0.8750\n"]}],"source":["history = model.fit(\n","    get_training_dataset(),\n","    steps_per_epoch=CFG.STEPS_PER_EPOCH,\n","    epochs=CFG.EPOCHS,\n","    validation_data=get_validation_dataset(),\n","    validation_steps=CFG.VALIDATION_STEPS,\n","    callbacks=make_callbacks(CFG)\n",")"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["pretrained_model.save(CFG.ROOT / '../models' / CFG.MODEL / f\"{save_time}.h5\", save_format='h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQzOguVCg16L"},"outputs":[],"source":["art = wandb.Artifact(\n","    'model',\n","    type='model')\n","art.add_file(str(CFG.ROOT / 'models' / CFG.MODEL / f\"{time}.h5\"))\n","wandb.log_artifact(art)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"executionInfo":{"elapsed":1578,"status":"ok","timestamp":1697364201212,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"qfCXrWuFNo77","outputId":"7826babb-7c86-448c-a098-bee428d54fa9"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-6-0b107be7bb99>:92: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n","  ax = plt.subplot(subplot)\n","/usr/local/lib/python3.10/dist-packages/plotly/matplotlylib/renderer.py:647: UserWarning:\n","\n","Looks like the annotation(s) you are trying \n","to draw lies/lay outside the given figure size.\n","\n","Therefore, the resulting Plotly figure may not be \n","large enough to view the full text. To adjust \n","the size of the figure, use the 'width' and \n","'height' keys in the Layout object. Alternatively,\n","use the Margin object to adjust the figure's margins.\n","\n","/usr/local/lib/python3.10/dist-packages/plotly/matplotlylib/renderer.py:611: UserWarning:\n","\n","I found a path object that I don't think is part of a bar chart. Ignoring.\n","\n"]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n","display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7a95e6bcfca34c5ca67d863895380872","88960c221ba443f5b10a8a63dc773217","e249c6d64080430ca273dacb2cae446f","b707fc265eb1412ebb8d455d32b609d0","d397f7b7138e49a8a6801d96e7a8549b","dc5a275727a745b9851b68716f123861","ccb468c6897448c5ad68ea166f78f027","1c3cc56bb26b46bdab17e364d28c66ed"]},"executionInfo":{"elapsed":6494,"status":"ok","timestamp":1697364207703,"user":{"displayName":"graham broughton","userId":"15728648374086258761"},"user_tz":420},"id":"EP-fMeipNfKG","outputId":"b91b04b5-c590-4025-8cee-f42d5f13a8d4"},"outputs":[{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a95e6bcfca34c5ca67d863895380872","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='4498.752 MB of 4498.752 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/batch_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>batch/learning_rate</td><td>██▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>███▆▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>batch/lr</td><td>██▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/sparse_categorical_accuracy</td><td>▁▁▁▂▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████▇▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>batch/sparse_top_3_categorical_accuracy</td><td>▁▁▁▃▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████▇▇▇▇▇▇▇▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▃▂▂▂▂▂▂▁▁▁▁▂▂▂▂▁▁▁</td></tr><tr><td>epoch/lr</td><td>█▇▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/sparse_categorical_accuracy</td><td>▁▃▅▆▇▇▇▇▇▇██▇▇▇▇▇▇██</td></tr><tr><td>epoch/sparse_top_3_categorical_accuracy</td><td>▁▄▆▇▇▇▇▇▇████▇▇▇▇███</td></tr><tr><td>epoch/val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_lr</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_sparse_categorical_accuracy</td><td>▁▄▆█████████████████</td></tr><tr><td>epoch/val_sparse_top_3_categorical_accuracy</td><td>▁▆▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/batch_step</td><td>3779</td></tr><tr><td>batch/learning_rate</td><td>0.0</td></tr><tr><td>batch/loss</td><td>0.3781</td></tr><tr><td>batch/lr</td><td>0.0</td></tr><tr><td>batch/sparse_categorical_accuracy</td><td>0.90427</td></tr><tr><td>batch/sparse_top_3_categorical_accuracy</td><td>0.97718</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/learning_rate</td><td>0.0</td></tr><tr><td>epoch/loss</td><td>0.3781</td></tr><tr><td>epoch/lr</td><td>0.0</td></tr><tr><td>epoch/sparse_categorical_accuracy</td><td>0.90427</td></tr><tr><td>epoch/sparse_top_3_categorical_accuracy</td><td>0.97718</td></tr><tr><td>epoch/val_loss</td><td>1.08444</td></tr><tr><td>epoch/val_lr</td><td>0.0</td></tr><tr><td>epoch/val_sparse_categorical_accuracy</td><td>0.72266</td></tr><tr><td>epoch/val_sparse_top_3_categorical_accuracy</td><td>0.875</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">distinctive-star-42</strong> at: <a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/ovp3we8x' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier/runs/ovp3we8x</a><br/>Synced 5 W&B file(s), 2 media file(s), 20 artifact file(s) and 1 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20231015_092439-ovp3we8x/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpdijuMKjUo7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1iUOXvvXCq1WNVkG_PUA9YWR5SgaNB6ww","timestamp":1697270080331}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1c3cc56bb26b46bdab17e364d28c66ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a95e6bcfca34c5ca67d863895380872":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_88960c221ba443f5b10a8a63dc773217","IPY_MODEL_e249c6d64080430ca273dacb2cae446f"],"layout":"IPY_MODEL_b707fc265eb1412ebb8d455d32b609d0"}},"88960c221ba443f5b10a8a63dc773217":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d397f7b7138e49a8a6801d96e7a8549b","placeholder":"​","style":"IPY_MODEL_dc5a275727a745b9851b68716f123861","value":"4498.752 MB of 4498.795 MB uploaded (0.000 MB deduped)\r"}},"b707fc265eb1412ebb8d455d32b609d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccb468c6897448c5ad68ea166f78f027":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d397f7b7138e49a8a6801d96e7a8549b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc5a275727a745b9851b68716f123861":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e249c6d64080430ca273dacb2cae446f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccb468c6897448c5ad68ea166f78f027","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c3cc56bb26b46bdab17e364d28c66ed","value":0.9999903538631951}}}}},"nbformat":4,"nbformat_minor":0}
