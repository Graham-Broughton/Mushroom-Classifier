{"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7315295,"sourceType":"datasetVersion","datasetId":4245032},{"sourceId":7325978,"sourceType":"datasetVersion","datasetId":4252138},{"sourceId":7419892,"sourceType":"datasetVersion","datasetId":4316787},{"sourceId":157263654,"sourceType":"kernelVersion"},{"sourceId":158163589,"sourceType":"kernelVersion"},{"sourceId":158208034,"sourceType":"kernelVersion"},{"sourceId":147,"sourceType":"modelInstanceVersion","modelInstanceId":100},{"sourceId":490,"sourceType":"modelInstanceVersion","modelInstanceId":367},{"sourceId":493,"sourceType":"modelInstanceVersion","modelInstanceId":370},{"sourceId":496,"sourceType":"modelInstanceVersion","modelInstanceId":373},{"sourceId":500,"sourceType":"modelInstanceVersion","modelInstanceId":377},{"sourceId":622,"sourceType":"modelInstanceVersion","modelInstanceId":489},{"sourceId":630,"sourceType":"modelInstanceVersion","modelInstanceId":497},{"sourceId":3817,"sourceType":"modelInstanceVersion","modelInstanceId":2720},{"sourceId":3819,"sourceType":"modelInstanceVersion","modelInstanceId":2722}],"dockerImageVersionId":30628,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -q wandb keras_cv python-dotenv tensorflow-addons==0.22.0 # tensorboard_plugin_profile==2.8.0 ","metadata":{"id":"_ga4wuA-bnHH","outputId":"5422ba31-8db3-411d-bc4f-453c0c3f8828","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RERUN = False\nif RERUN:\n    WANDB_ID = \"\"\n    MODEL_VERSION = \"\"  # v[0-9]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math, re, os, pickle\nfrom typing import Any, Dict, List, Optional\nfrom datetime import datetime\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, Sequential\nimport keras_cv\nimport tensorflow_addons as tfa\nimport tensorflow_hub as hub\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weights\nimport wandb\nfrom wandb.keras import WandbCallback, WandbMetricsLogger\nfrom wandb_callback import WandbClfEvalCallback\nfrom config import CFG\nimport gc\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport warnings\nwarnings.simplefilter('ignore')\nprint(f\"Tensorflow version {tf.__version__}\")\nAUTO = tf.data.experimental.AUTOTUNE\n\nCFG = CFG()\n\nroot = Path('kaggle')\ninp = root / 'input'\nout = root / working","metadata":{"id":"LfAu_aLnNo7l","outputId":"9932930b-7e4e-4422-8037-05453a6d306f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)\nCFG.GCS_REPO = user_secrets.get_secret(\"GCS_REPO\")\nos.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(CFG.SEED)\ntf.random.set_seed(CFG.SEED)\nos.environ['TF_CUDNN_DETERMINISTIC'] = '1'\nos.environ['PYTHONHASHSEED'] = str(CFG.SEED)","metadata":{"id":"dVzlCFu1N5Dd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\nexcept ValueError:  # If TPU not found\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\nCFG.BATCH_SIZE = CFG.BASE_BATCH_SIZE * REPLICAS  # CFG.BASE_BATCH_SIZE * REPLICAS","metadata":{"id":"PBI-WAppNo7s","outputId":"284b5298-cadb-4f04-f80f-0c6c15317e31","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"REPLICAS = strategy.num_replicas_in_sync\nCFG.BATCH_SIZE = CFG.BASE_BATCH_SIZE * REPLICAS  # CFG.BASE_BATCH_SIZE * REPLICAS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization Utils","metadata":{"id":"nnHHCJk-No7v"}},{"cell_type":"code","source":"def batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return class_dict[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(class_dict[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                class_dict[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    image = (image - image.min()) / (\n        image.max() - image.min()\n    )  # convert to [0, 1] for avoiding matplotlib warning\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    (images), (images, predictions), ((images, labels)), ((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n\n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n\n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n\n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else class_dict[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n\n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n\ndef display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(class_dict)))\n    ax.set_xticklabels(class_dict, fontdict={'fontsize': 7})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(class_dict)))\n    ax.set_yticklabels(class_dict, fontdict={'fontsize': 7})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if titlestring != \"\":\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title(f'model {title}')\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","metadata":{"id":"yTKGP-V2No7x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1))\n         for filename in filenames]\n    return np.sum(n)\n\ndef decode_image(image_data, smallest_side, CFG):\n    image = tf.image.decode_jpeg(image_data, channels=3)  # image format uint8 [0,255]\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize_with_crop_or_pad(image, smallest_side, smallest_side)\n    image = tf.image.resize(image, size=CFG.PRECROP_SIZE, method=\"lanczos5\")\n    image = tf.image.random_crop(image, size=[*CFG.CROP_SIZE, 3])  #, method=\"lanczos5\"\n    return image\n\ndef read_labeled_tfrecord(example, CFG, return_id):\n    feature_description = {\n        \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n        \"image/id\": tf.io.FixedLenFeature([], tf.string),\n        \"image/meta/width\": tf.io.FixedLenFeature([], tf.int64),\n        \"image/meta/height\": tf.io.FixedLenFeature([], tf.int64),\n        \"image/class/label\": tf.io.FixedLenFeature([], tf.int64),\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n\n    width = tf.cast(example['image/meta/width'], tf.int32)\n    height = tf.cast(example['image/meta/height'], tf.int32)\n    smallest_side = tf.minimum(width, height)\n\n    image = decode_image(example[\"image/encoded\"], smallest_side, CFG)\n    label = tf.cast(example[\"image/class/label\"], tf.int32)\n\n    if not return_id:\n        return image, label\n\n    id = tf.cast(example['image/id'], tf.string)\n    return image, label, id\n\ndef load_dataset(filenames, CFG, order=False, return_id=False):\n    \"\"\"Read from TFRecords. For optimal performance, read from multiple\n    TFRecord files at once and set the option experimental_deterministic = False\n    to allow order-altering optimizations.\"\"\"\n    if not order:\n        option_no_order = tf.data.Options()\n        option_no_order.experimental_deterministic = False\n        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        dataset = dataset.with_options(option_no_order)\n    else:\n        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=None)\n\n    dataset = dataset.map(lambda x: read_labeled_tfrecord(x, CFG, return_id), num_parallel_calls=AUTO)\n    return dataset\n\ndef basic_augment(image, CFG=CFG):\n    \"\"\"Apply various augmentations to an image.\n\n    Args:\n        image: The input image.\n        prob_hflip: Probability of applying horizontal flip.\n        prob_vflip:\n        prob_color_jitter: Probability of applying color jitter.\n        prob_rotate: Probability of applying rotation.\n\n    Returns:\n        Augmented image.\n    \"\"\"\n    num = tf.random.uniform([])\n    # Horizontal Flip\n    if num < CFG.FLIP:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n\n    # Color Jitter\n    elif num < CFG.JITTER:\n        image = tf.image.random_brightness(image, max_delta=0.9)#CFG.JITTER_BRIGHT_LIM)\n        image = tf.image.random_contrast(image, 0.6, 1.4)#*CFG.JITTER_CONTRAST_LIMS)\n        image = tf.image.random_saturation(image, 0.6, 1.4)#*CFG.JITTER_SAT_LIMS)\n        image = tf.image.random_hue(image, max_delta=0.15)#CFG.JITTER_HUE_LIM)\n\n    # Rotation\n    elif num < CFG.ROTATE:\n        angles = tf.random.uniform([], *CFG.ROTATE_LIM)\n        image = tfa.image.rotate(image, angles, fill_mode='reflect')\n\n    else:\n        image = random_masking(image)\n    return image\n\ndef random_masking(image, CFG=CFG):\n    original_shape = tf.shape(image)\n\n    count = tf.random.uniform([], *CFG.COUNT)\n\n    erase_size = tf.random.uniform([], *CFG.SIZE)\n    erase_size = tf.math.divide(erase_size, tf.math.sqrt(count))\n    erase_value = tf.random.uniform([], 0., 255.)\n\n    mask1 = int(erase_size * float(original_shape[0])) - (int(erase_size * float(original_shape[0])) % 2)\n    mask2 = int(erase_size * float(original_shape[1])) - (int(erase_size * float(original_shape[1])) % 2)\n\n    image = tf.expand_dims(image, axis=0)\n    for k in tf.range(count):\n        image = tfa.image.random_cutout(\n            image,\n            mask_size=(mask1, mask2),\n            constant_values=erase_value\n        )\n    image = tf.squeeze(image, 0)\n    return image\n\ndef augment(images, labels, CFG):\n    images = tf.map_fn(basic_augment, images)\n    # images = tf.map_fn(random_masking, images)\n    return images, labels\n\nrand_augment = keras_cv.layers.RandAugment(\n    value_range=(0, 255), augmentations_per_image=CFG.PER_IMAGE, magnitude=CFG.MAG\n)\n\ndef get_batched_dataset(filenames, CFG, train=False, order=None):\n    if order is None:\n        order = not train\n    dataset = load_dataset(filenames, CFG, order=order)\n    # dataset = dataset.cache() # This dataset fits in RAM\n    dataset = dataset.repeat()\n    dataset = dataset.batch(CFG.BATCH_SIZE, drop_remainder=True)\n    if train & CFG.AUGMENT:\n#         dataset = dataset.map(lambda x, y: augment(x, y, CFG), num_parallel_calls=AUTO)\n        dataset = dataset.map(\n            lambda x, y: (\n                rand_augment(tf.cast(x, tf.uint8)), y),\n                num_parallel_calls=AUTO,\n            )\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","metadata":{"id":"8ynfISf3No7z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# CFG.EPOCHS = 30\n\nGCS_PATH_SELECT = {\n    192: f'gs://{CFG.GCS_REPO}/tfrecords-jpeg-192x192',\n    224: f'gs://{CFG.GCS_REPO}/tfrecords-jpeg-224x224',\n    331: f'gs://{CFG.GCS_REPO}/tfrecords-jpeg-331x331',\n    512: f'gs://{CFG.GCS_REPO}/tfrecords-jpeg-512x512',\n    None: f'gs://{CFG.GCS_REPO}/tfrecords-jpeg-MO3'\n}\nGCS_PATH = GCS_PATH_SELECT[CFG.GCS_IMAGE_SIZE]\n\nfilenames = tf.io.gfile.glob(f\"{GCS_PATH}/train*.tfrec\")\n\nif RERUN:\n    training_filenames = set(filenames) - set(validation_filenames) - set(test_filenames)\n    trining_filenames = list(training_filenames)\nelse:\n    filenames, test_filenames = train_test_split(filenames, test_size=1, shuffle=True)\n    training_filenames, validation_filenames = train_test_split(filenames, test_size=0.1, shuffle=True)\n\nnum_train = count_data_items(training_filenames)\nnum_val = count_data_items(validation_filenames)\nnum_test = count_data_items(test_filenames)\n\nvalidation_steps = num_val / CFG.BATCH_SIZE // REPLICAS\nsteps_per_epoch = num_train / CFG.BATCH_SIZE // REPLICAS\nTOTAL_STEPS = int(steps_per_epoch * (CFG.EPOCHS - 1))\n\nclass_dict = pickle.load(open(inp / 'class-dict/class_dict_NEW3.pkl', 'rb'))\ntrain_df = pd.read_csv(inp / 'train-csv/train_with_MO3.csv')","metadata":{"id":"AXjXcpF5No7t","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.DEBUG:\n    # Peek at training data\n    training_dataset = get_batched_dataset(training_filenames, CFG, train=True)\n    training_dataset = training_dataset.unbatch().batch(20)\n    train_batch = iter(training_dataset)","metadata":{"id":"SYJ_luSVNo74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.DEBUG:\n    # run this cell again for next set of images\n    display_batch_of_images(next(train_batch))","metadata":{"id":"LvzhBMQ_No75","outputId":"52409d89-bc49-4d2a-e3d5-c04d2b334f75","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.DEBUG:\n    print(\"Training data shapes:\")\n    for image, label in get_batched_dataset(training_filenames, CFG, train=True).take(3):\n        print(image.numpy().shape, label.numpy().shape)\n    print(\"Training data label examples:\", label.numpy())\n    print(\"Validation data shapes:\")\n    for image, label in get_batched_dataset(validation_filenames, CFG).take(3):\n        print(image.numpy().shape, label.numpy().shape)\n    print(\"Validation data label examples:\", label.numpy())","metadata":{"id":"cC5l84OONo73","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match CFG.MODEL:\n    case 'swin-224':\n        model_path = '/kaggle/input/swin/tensorflow2/large-patch4-window7-224-fe/1'\n    case 'swin-384':\n        model_path = '/kaggle/input/swin/tensorflow2/large-patch4-window12-384-fe/1'\n    case 'convnext-large-21k-224':\n        model_path = '/kaggle/input/convnext/tensorflow2/large-21k-1k-224-fe/1'\n    case 'convnext-large-1k-224':\n        model_path = '/kaggle/input/convnext/tensorflow2/large-1k-224-fe/1'\n    case 'convnext-xlarge-21k-224':\n        model_path = '/kaggle/input/convnext/tensorflow2/xlarge-21k-1k-224-fe/1'\n    case 'convnext-xlarge-21k-384':\n        model_path = '/kaggle/input/convnext/tensorflow2/xlarge-21k-1k-384-fe/1'\n    case 'vit-l16-fe':\n        model_path = '/kaggle/input/vision-transformer/tensorflow2/vit-l16-fe/1'\n    case 'vit-b8-fe':\n        model_path = '/kaggle/input/vision-transformer/tensorflow2/vit-b8-fe/1'\n    case 'bit':\n        model_path = '/kaggle/input/bit/tensorflow2/m-r152x4/1'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RERUN:\n    try:\n        with strategy.scope():\n            options = tf.saved_model.LoadOptions(\n                    experimental_io_device=\"/job:localhost\"\n                )\n            final_model = tf.keras.models.load_model(f'/content/drive/MyDrive/Mushroom-Classifier/artifacts/run_{WANDB_ID}_model:{MODEL_VERSION}', options=options)\n\n    except FileNotFoundError:\n        artifact = run.use_artifact(f'g-broughton/Mushroom-Classifier/run_{WANDB_ID}_model:{MODEL_VERSION}', type='model')\n        artifact_dir = artifact.download()\n\n        with strategy.scope():\n            options = tf.saved_model.LoadOptions(\n                    experimental_io_device=\"/job:localhost\"\n                )\n            final_model = tf.keras.models.load_model(artifact_dir, options=options)\n\n\nelse:\n    with strategy.scope():\n        img_adjust_layer = layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode=\"torch\"), input_shape=[*CFG.CROP_SIZE, 3])\n        pretrained_model = tf.keras.models.load_model(model_path)\n\n        model = Sequential([\n            img_adjust_layer,\n            pretrained_model,\n        ])\n\n        # Assuming input_image is the input layer for model\n        input_image = layers.Input(shape=model.input_shape[1:])\n        model_output = model(input_image)\n        output = layers.Dense(len(class_dict), activation='softmax')(model_output)\n\n        # Create the final model\n        final_model = Model(inputs=input_image, outputs=output)\n\n        top3 = tf.keras.metrics.SparseTopKCategoricalAccuracy(3, name='top-3-accuracy')\n\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['class_id']), y=train_df['class_id'])\n    final_model.compile(\n        optimizer=tf.keras.optimizers.AdamW(\n            learning_rate=.001,\n            epsilon=CFG.EPSILON,\n            weight_decay=CFG.DECAY,\n            beta_1=CFG.BETA1,\n            beta_2=CFG.BETA2\n        ),\n        loss = tf.keras.losses.CategoricalFocalCrossentropy(alpha=class_weights, label_smoothing=0.1)  # 'sparse_categorical_crossentropy',\n        metrics=['accuracy', top3]\n    )\n    final_model.summary()","metadata":{"id":"rPcaVRUTNo76","outputId":"ed485d3b-2811-4f8e-b102-86dab959d16c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RERUN:\n    run = wandb.init(\n        project=\"Mushroom-Classifier\",\n        tags=[CFG.OPT, CFG.LR_SCHED, str(CFG.CROP_SIZE[0])],\n        resume='allow',\n        id=WANDB_ID,\n        dir=\"../\",\n    )\n\n    validation_filenames = run.config['VAL_FILENAMES']\n    test_filenames = run.config['TEST_FILENAMES']\nelse:\n    config = wandb.helper.parse_config(\n        CFG, include=(\n            'SEED', 'BATCH_SIZE', 'EPOCHS', 'PRECROP_SIZE', 'LR_START', 'with_MO',\n            'LR_MAX', 'LR_RAMPEP', 'LR_SUSEP', 'LR_MIN', 'EPSILON', 'DECAY', 'BETA1', 'BETA2',\n            # 'JITTER_BRIGHT_LIM', 'JITTER_CONTRAST_LIMS', 'JITTER_SAT_LIMS', 'JITTER_HUE_LIM', 'ROTATE_LIM', 'SIZE', 'COUNT',\n            'LR_SCHED', 'MODEL', 'MAG', 'PER_IMAGE' 'LR_DECAY',\n        )\n    )\n    wandb.init(\n        # reinit=True,\n        project=\"Mushroom-Classifier\",\n        tags=[CFG.OPT, CFG.LR_SCHED, str(CFG.CROP_SIZE[0])],\n        config=config,\n        dir=\"../\",\n    )\n    wandb.config.update({\n        \"STEPS_PER_EPOCH\": steps_per_epoch,\n        \"TOTAL_STEPS\": TOTAL_STEPS,\n        \"TEST_FILENAMES\": test_filenames,\n        \"VAL_FILENAMES\": validation_filenames\n        'LOSS': 'focal'\n        'LABEL_SMOOTHING': 0.1\n        \"ALPHA\": \"sklearn compute class weights\"\n        \"GAMMA\": 2\n    })","metadata":{"id":"n4zBTS8hbnYC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CFG.LR_SUSEP = 0\n# CFG.LR_DECAY = 0.85\n# CFG.LR_MIN = 0.000001\n# CFG.LR_MAX = 0.000001\n\ndef get_lr_callback(batch_size=8, plot=False):\n    lr_start   = CFG.LR_START  # 0.000001\n    lr_max     = CFG.LR_MAX * batch_size #CFG.LR_MAX * batch_size\n    lr_min     = CFG.LR_MIN  # 0.0000001\n    lr_ramp_ep = CFG.LR_RAMPEP  # 5\n    lr_sus_ep  = CFG.LR_SUSEP  # 0\n    lr_decay   = CFG.LR_DECAY  # 0.9\n    # CFG.LR_MAX = lr_max\n\n    def lrfn(epoch):\n        if epoch <= lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n\n        elif CFG.LR_SCHED=='ExponentialWarmup':\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n\n        elif CFG.LR_SCHED=='CosineWarmup':\n            decay_total_epochs = CFG.EPOCHS - lr_ramp_ep - lr_sus_ep\n            decay_epoch_index = epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            cosine_decay = 0.4 * (1 + math.cos(phase))\n            lr = (lr_max - lr_min) * cosine_decay + lr_min\n\n        return lr\n\n    if plot:\n        plt.figure(figsize=(10,5))\n        plt.plot(np.arange(CFG.EPOCHS), [lrfn(epoch) for epoch in np.arange(CFG.EPOCHS)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('learning rate')\n        plt.title('Learning Rate Scheduler')\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\n_=get_lr_callback(CFG.BATCH_SIZE, plot=True )","metadata":{"id":"hpO6YD4PP-i5","outputId":"b24770cf-eb50-40d0-a5b3-90987e2c11fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options = tf.saved_model.SaveOptions(\n        experimental_io_device=\"/job:localhost\"\n    )\nval_data = load_dataset(validation_filenames, CFG, order=True).take(51200).batch(1024).prefetch(AUTO)\n\nmodeldir = out / 'model'\nmodeldir.mkdir(exist_ok=True, parents=True)\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_accuracy\",\n        patience=CFG.ES_PATIENCE,\n        verbose=1,\n        mode=\"max\",\n        restore_best_weights=True,\n    ),\n    wandb.keras.WandbMetricsLogger(log_freq=\"epoch\"),\n#     wandb.keras.WandbModelCheckpoint(\n#         str(modeldir),#CFG.CKPT_DIR,  # .h5 for weights, dir for whole model\n#         monitor=\"val_accuracy\",\n#         mode=\"max\",\n#         verbose=1,\n#         save_best_only=True,\n#         save_weights_only=False,\n#         options=options,\n#         initial_value_threshold=0.8,\n#         save_freq=TOTAL_STEPS,\n#     ),\n    get_lr_callback(CFG.BATCH_SIZE),\n    if CFG.RERUN:\n        WandbClfEvalCallback(\n            validation_data=val_data,\n            data_table_columns=[\"idx\", \"image\", \"species\"],\n            pred_table_columns=[\"image\", \"species\", \"predicted species\", \"score\"],\n            num_batches=50,\n            class_dict=class_dict\n        ),\n\n    # tf.keras.callbacks.TensorBoard(\n    #     log_dir = logs,\n    #     histogram_freq = 1,\n    #     # profile_batch = '500,520'\n    # )\n]","metadata":{"id":"EY8p9ckjW32L","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RERUN:\n    tfiles = training_filenames[0]\n    history = final_model.fit(\n        get_batched_dataset(tfiles, CFG, train=True).unbatch().take(32).batch(32),\n        epochs=CFG.EPOCHS,\n        validation_data=get_batched_dataset(validation_filenames, CFG, train=False),\n        validation_steps=validation_steps,\n        callbacks=callbacks,\n        initial_epoch=CFG.EPOCHS,\n    )\nelse:\n    history = final_model.fit(\n        get_batched_dataset(training_filenames, CFG, train=True),\n        steps_per_epoch=steps_per_epoch,\n        epochs=CFG.EPOCHS,\n        validation_data=get_batched_dataset(validation_filenames, CFG, train=False),\n        validation_steps=validation_steps,\n        callbacks=callbacks,\n    )\n# wandb.finish()","metadata":{"id":"fmM3Sf5ONo77","outputId":"172aa81e-23e2-4d12-dab8-fc928f833cf0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.save(modeldir)\n\nmodel_checkpoint_artifact = wandb.Artifact(\n    f\"run_{wandb.run.id}_model\", type=\"model\"\n)\nif os.path.isdir('/kaggle/working/model'):\n    model_checkpoint_artifact.add_dir(modeldir)\nelse:\n    raise FileNotFoundError(f\"No such file or directory {modeldir}\")\nwandb.log_artifact(model_checkpoint_artifact, aliases='latest')\n\n# wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get image ID, Label and Model Predictions","metadata":{}},{"cell_type":"code","source":"def process_and_save_predictions(model, dataset, batch_size, num_samples):\n    predictions = []\n    actual_labels = []\n    score_list = []\n    id_list = []\n\n    for image_data, labels, ids in tqdm(dataset, total=(num_samples // batch_size)):\n        # Predict\n        preds = model.predict_on_batch(image_data)\n        pred_idx = tf.math.argmax(preds, axis=-1)\n        scores = tf.math.reduce_max(preds, axis=-1)\n\n        # Append predictions and labels\n        predictions.extend(pred_idx.numpy().tolist())\n        actual_labels.extend(labels.numpy().tolist())\n        score_list.extend(scores.numpy().tolist())\n        id_list.extend(ids.numpy().tolist())\n\n    # Create DataFrame\n    results_df = pd.DataFrame({\n        'Prediction': predictions,\n        'Actual Label': actual_labels,\n        'Score': score_list,\n        'ids': id_list,\n    })\n\n    results_df['Species'] = results_df['Actual Label'].map(class_dict)\n    results_df['Species Pred'] = results_df['Prediction'].map(class_dict)\n    return results_df","metadata":{"id":"T7Zeuy2FbG8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filenames, num, csvname, batch in zip([test_filenames, training_filenames, validation_filenames], [num_test, num_train, num_val], [\"test_df.csv\", \"train_df.csv\", \"val_df.csv\"], [256, 1024, 1024]):\n    ds = load_dataset(filenames, CFG, return_id=True).batch(batch).prefetch(AUTO)\n    df = process_and_save_predictions(final_model, ds, batch, num)\n    table = wandb.Table(dataframe=df)\n    wandb.log({csvname: table})\n    df.to_csv(out / csvname, index=False)\n\nwandb.finish()","metadata":{"id":"1YQJEcnpSCRp","outputId":"8e64e123-cdc2-4293-c557-b0b45ca3efac"},"execution_count":null,"outputs":[]}]}