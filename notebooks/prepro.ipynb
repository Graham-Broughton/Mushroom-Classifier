{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from training.train_config import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_priors(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Calculates the class priors for a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the class labels.\n",
    "\n",
    "    Returns:\n",
    "        class_priors (np.ndarray): An array containing the class priors.\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating class priors\")\n",
    "    class_priors = np.zeros(len(df[\"class_id\"].unique()))\n",
    "    for species in df[\"class_id\"].unique():\n",
    "        class_priors[species] = len(df[df[\"class_id\"] == species])\n",
    "\n",
    "    return class_priors / sum(class_priors)\n",
    "\n",
    "\n",
    "def month_distributions(df):\n",
    "    \"\"\"Calculates the distribution of mushroom classes for each month in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame containing the mushroom data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the distribution of mushroom classes for each month.\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating month distributions\")\n",
    "    month_distributions = {}\n",
    "\n",
    "    for _, observation in tqdm(df.iterrows(), total=len(df)):\n",
    "        month = str(observation[\"date\"].month)\n",
    "        if month not in month_distributions:\n",
    "            month_distributions[month] = np.zeros(len(df[\"class_id\"].unique()))\n",
    "        else:\n",
    "            class_id = observation.class_id\n",
    "            month_distributions[month][class_id] += 1\n",
    "\n",
    "    for key, value in month_distributions.items():\n",
    "        month_distributions[key] = value / sum(value)\n",
    "    return month_distributions\n",
    "\n",
    "\n",
    "def parse_json(filepath, is_test=False, categories=None):\n",
    "    \"\"\"Parses a JSON file and returns relevant dataframes.\n",
    "\n",
    "    Args:\n",
    "        filepath (pathlib.Path): The path to the JSON file.\n",
    "        is_test (bool, optional): Whether the JSON file is a test file. Defaults to False.\n",
    "        categories (DataFrame, optional): A dataframe containing categories. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A dataframe containing information.\n",
    "        DataFrame: A dataframe containing images.\n",
    "        DataFrame: A dataframe containing annotations (if not a test file).\n",
    "        DataFrame: A dataframe containing categories (if categories parameter is not None and not a test file).\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        res = json.load(f)\n",
    "    info = pd.DataFrame.from_dict(res[\"info\"], orient=\"index\")\n",
    "    images = pd.DataFrame(res[\"images\"]).set_index(\"id\")\n",
    "    if not is_test:\n",
    "        annotations = pd.DataFrame(res[\"annotations\"]).set_index(\"id\")\n",
    "        if categories:\n",
    "            categories = pd.DataFrame(res[\"categories\"]).set_index(\"id\")\n",
    "            return info, images, annotations, categories\n",
    "        return info, images, annotations\n",
    "\n",
    "    return info, images\n",
    "\n",
    "\n",
    "def join_dataframes(images, annotations, categories, dset=None, locations=None):\n",
    "    \"\"\"Join dataframes containing information about images, annotations, categories, and locations (optional).\n",
    "    Only categories with the supercategory 'Fungi' are included.\n",
    "\n",
    "    Args:\n",
    "        images (DataFrame): dataframe containing information about images\n",
    "        annotations (DataFrame): dataframe containing information about annotations\n",
    "        categories (DataFrame): dataframe containing information about categories\n",
    "        locations (DataFrame, optional): dataframe containing information about image locations\n",
    "\n",
    "    Returns:\n",
    "        df (DataFrame): merged dataframe with selected columns dropped\n",
    "    \"\"\"\n",
    "    categories = categories[categories[\"supercategory\"] == \"Fungi\"].rename(\n",
    "        columns={\"id\": \"category_id\"}\n",
    "    )\n",
    "    if locations is None:  # some datasets do not have location information\n",
    "        df = pd.merge(\n",
    "            categories, annotations, right_on=\"category_id\", left_index=True\n",
    "        ).merge(images, left_on=\"image_id\", right_index=True)\n",
    "    else:\n",
    "        df = pd.merge(annotations, categories, on=\"category_id\").set_index(\"image_id\")\n",
    "        df = df.merge(images, left_index=True, right_index=True)\n",
    "        df = df.merge(locations, right_index=True, left_index=True)\n",
    "        \n",
    "    df = df.drop(\n",
    "        [\"supercategory\", \"kingdom\", \"image_id\", \"valid\", \"license\", \"rights_holder\"],\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "    if dset is not None:\n",
    "        df[\"dset\"] = dset\n",
    "    return df\n",
    "\n",
    "\n",
    "# @flow(name='Parse2018Data')\n",
    "def parse_2018_data(data_root):\n",
    "    \"\"\"Parses the 2018 mushroom dataset from the given data root directory.\n",
    "\n",
    "    Args:\n",
    "        data_root (pathlib.Path): The root directory of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A dataframe containing the parsed data.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Parsing 2018 data from {data_root}\")\n",
    "\n",
    "    # Parse categories\n",
    "    with open(data_root / \"categories.json\", \"r\") as f:\n",
    "        cats = pd.DataFrame(json.load(f))\n",
    "\n",
    "    # Parse train and validation data\n",
    "    (timages2018, tanno2018), (vimages2018, vanno2018) = [\n",
    "        parse_json(data_root / f\"{s}2018.json\")[1:]\n",
    "        for s in [\"train\", \"val\"]\n",
    "    ]\n",
    "\n",
    "    # Parse train and validation locations\n",
    "    tloc, vloc = [\n",
    "        pd.read_json(data_root / \"inat2018_locations\" / f\"{s}2018_locations.json\").set_index(\"id\")\n",
    "        for s in [\"train\", \"val\"]\n",
    "    ]\n",
    "\n",
    "    # Join dataframes and save which dset they are from\n",
    "    val = join_dataframes(vimages2018, vanno2018, cats, locations=vloc, dset=\"val\")\n",
    "    train = join_dataframes(timages2018, tanno2018, cats, locations=tloc, dset=\"train\")\n",
    "    df = pd.concat([train, val]).reset_index(drop=True)\n",
    "    df[\"dataset\"] = \"2018\"\n",
    "\n",
    "    # Create new directories and paths\n",
    "    # df[\"file_name\"] = df[\"file_name\"].str.split(\"/\").str[-1]\n",
    "    df[\"specific_epithet\"] = df[\"name\"].str.split().str[-1]\n",
    "    # df[\"image_dir_name\"] = df[\n",
    "    #     [\"phylum\", \"class\", \"order\", \"family\", \"genus\", \"specific_epithet\"]\n",
    "    # ].apply(lambda x: f\"Fungi_{'_'.join(x)}\", axis=1)\n",
    "\n",
    "    # Drop unneeded columns and rename others\n",
    "    df = df.drop([\"category_id\", \"date_c\"], axis=1).rename(\n",
    "        columns={\n",
    "            \"lon\": \"longitude\",\n",
    "            \"lat\": \"latitude\",\n",
    "            \"loc_uncert\": \"location_uncertainty\",\n",
    "        }\n",
    "    )\n",
    "    logger.debug(f\"2018 dataframe shape {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# @flow(name='Parse2021Data')\n",
    "def parse_2021_data(data_root):\n",
    "    \"\"\"Parses 2021 mushroom data from the given data root directory.\n",
    "\n",
    "    Args:\n",
    "        data_root (pathlib.Path): The root directory of the 2021 mushroom data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A concatenated dataframe of the parsed mushroom data.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Parsing 2021 data from {data_root}\")\n",
    "    sets = [\"train\", \"val\"]\n",
    "\n",
    "    dfs = [\n",
    "        join_dataframes(\n",
    "            *parse_json(data_root / f\"{s}.json\", categories=True)[1:], dset=s\n",
    "        )\n",
    "        for s in sets\n",
    "    ]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    df[\"dataset\"] = \"2021\"\n",
    "    df[\"file_name_mod\"] = df[\"file_name\"].str.split(\"/\").str[-1]\n",
    "    # df[\"image_dir_name_mod\"] = df[\"image_dir_name\"].apply(\n",
    "    #     lambda x: \"_\".join(x.split(\"_\")[1:])\n",
    "    # )\n",
    "    df = df.drop([\"category_id\", \"common_name\"], axis=1)\n",
    "    logger.debug(f\"2021 dataframe shape {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# @flow(name='JoinDatasets')\n",
    "def join_datasets(CFG, root) -> tuple:\n",
    "    \"\"\"Join two mushroom datasets, parse date column, create file path and GCS path columns,\n",
    "    create class ID column, calculate month distribution and class prior, and return the\n",
    "    concatenated dataframe and month distribution as a tuple.\n",
    "\n",
    "    Args:\n",
    "        gcs_bucket (str): The name of the Google Cloud Storage bucket.\n",
    "        root (pathlib.Path): The root directory of the mushroom datasets.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the concatenated dataframe and month distribution.\n",
    "    \"\"\"\n",
    "    df1 = parse_2018_data(root / \"2018\")\n",
    "    df2 = parse_2021_data(root / \"2021\")\n",
    "    logger.info(\"Joining all datasets\")\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"mixed\", utc=True)\n",
    "    # df[\"file_path\"] = (\n",
    "    #     str(CFG.DATA / df[\"image_dir_name\"] / df[\"file_name\"])\n",
    "    # )\n",
    "    df[\"gcs_path\"] = (\n",
    "        f\"gs://{CFG.GCS_REPO}/train/\" + df[\"image_dir_name\"] + \"/\" + df[\"file_name\"]\n",
    "    )\n",
    "    df[\"class_id\"] = df[\"name\"].astype(\"category\").cat.codes\n",
    "\n",
    "    month_distribution = month_distributions(df)\n",
    "    class_prior = class_priors(df)\n",
    "\n",
    "    df[\"class_priors\"] = df[\"class_id\"].map(dict(enumerate(class_prior)))\n",
    "\n",
    "    return df, month_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from os import environ\n",
    "    root = environ['PYTHONPATH']\n",
    "    raw_data_root = CFG.DATA / \"raw\"\n",
    "\n",
    "    df, month_distribution = join_datasets(CFG, raw_data_root)\n",
    "    \n",
    "    logger.debug(f\"Final dataframe shape {df.shape}\")\n",
    "    df.to_csv(CFG.DATA / \"train.csv\", index=False)\n",
    "\n",
    "    logger.info(\"Deleting unused images\")\n",
    "    total_filelist = raw_data_root.rglob('*.jpg')\n",
    "    total_fileset = set([x for x in total_filelist])\n",
    "\n",
    "    keep_set = set(df['file_name'].values.tolist())\n",
    "\n",
    "    files_to_delete = total_fileset - keep_set\n",
    "\n",
    "    for file in files_to_delete:\n",
    "        file.unlink()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
