{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 04:24:13.986887: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-29 04:24:13.992270: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-29 04:24:14.059361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-29 04:24:14.059404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-29 04:24:14.059449: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-29 04:24:14.070992: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-29 04:24:14.071908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-29 04:24:15.682899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import tensorflow.keras.backend as K\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "from os import environ\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "class_dict = pickle.load(open(\"../training/src/class_dict.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    BATCH_SIZE: int = 8\n",
    "    IMAGE_SIZE: tuple = (224, 224)\n",
    "    AUGMENT: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image_data, CFG):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)  # image format uint8 [0,255]\n",
    "    image = tf.reshape(image, [*CFG.IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_labeled_tfrecord(CFG, example):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'dataset': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'longitude': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'latitude': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'norm_date': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'class_priors': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'class_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = decode_image(example['image'], CFG)\n",
    "    label = tf.cast(example['class_id'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def load_dataset(filenames, CFG, labeled=True, ordered=False, shuffle=True):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # uses data as soon as it streams in, rather than in its original order\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    # dataset = dataset.cache()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(CFG.BATCH_SIZE * 10)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    if labeled:\n",
    "         dataset = dataset.map(lambda x: read_labeled_tfrecord(CFG, x), num_parallel_calls=AUTO) # if labeled else read_unlabeled_tfrecord\n",
    "    else:\n",
    "        dataset = dataset.map(lambda x: read_unlabeled_tfrecord(CFG, x), num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def data_augment(img, label, CFG):\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
    "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    img = transform(img, CFG)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    # img = tf.image.random_hue(img, 0.01)\n",
    "    img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "    img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "    img = tf.image.random_brightness(img, 0.1)\n",
    "    return img, label\n",
    "\n",
    "def get_training_dataset(filenames, CFG):\n",
    "    dataset = load_dataset(filenames, CFG, labeled=True)\n",
    "    if CFG.AUGMENT:\n",
    "        dataset = dataset.map(lambda x, y: data_augment(x, y, CFG), num_parallel_calls=AUTO)\n",
    "    # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.batch(CFG.BATCH_SIZE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(filenames, CFG, ordered=False):\n",
    "    dataset = load_dataset(filenames, CFG, labeled=True, ordered=ordered, shuffle=False)\n",
    "    dataset = dataset.batch(CFG.BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(filenames, CFG, ordered=False):\n",
    "    dataset = load_dataset(filenames, CFG, labeled=False, ordered=ordered, shuffle=False)\n",
    "    dataset = dataset.batch(CFG.BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'dataset': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'longitude': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'latitude': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'norm_date': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'class_priors': tf.io.FixedLenFeature([], tf.float32),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image']\n",
    "\n",
    "\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transform matrix which transforms indices\n",
    "\n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.0\n",
    "    shear = math.pi * shear / 180.0\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst], axis=0), [3, 3])\n",
    "\n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1], dtype='float32')\n",
    "    zero = tf.constant([0], dtype='float32')\n",
    "\n",
    "    rotation_matrix = get_3x3_mat([c1, s1, zero, -s1, c1, zero, zero, zero, one])\n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "\n",
    "    shear_matrix = get_3x3_mat([one, s2, zero, zero, c2, zero, zero, zero, one])\n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one / height_zoom, zero, zero, zero, one / width_zoom, zero, zero, zero, one])\n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one, zero, height_shift, zero, one, width_shift, zero, zero, one])\n",
    "\n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "\n",
    "def transform(image, CFG):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = CFG.IMAGE_SIZE[0]\n",
    "    XDIM = DIM % 2  # fix for size 331   \n",
    "\n",
    "    rot = CFG.ROT_ * tf.random.normal([1], dtype='float32')\n",
    "    shr = CFG.SHR_ * tf.random.normal([1], dtype='float32')\n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG.HZOOM_\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG.WZOOM_\n",
    "    h_shift = CFG.HSHIFT_ * tf.random.normal([1], dtype='float32')\n",
    "    w_shift = CFG.WSHIFT_ * tf.random.normal([1], dtype='float32')\n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot, shr, h_zoom, w_zoom, h_shift, w_shift)\n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat(tf.range(DIM // 2, -DIM // 2, -1), DIM)\n",
    "    y = tf.tile(tf.range(-DIM // 2, DIM // 2), [DIM])\n",
    "    z = tf.ones([DIM * DIM], dtype='int32')\n",
    "    idx = tf.stack([x, y, z])\n",
    "\n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM // 2 + XDIM + 1, DIM // 2)\n",
    "\n",
    "    # FIND ORIGIN PIXEL VALUES\n",
    "    idx3 = tf.stack([DIM // 2 - idx2[0,], DIM // 2 - 1 + idx2[1,]])\n",
    "    d = tf.gather_nd(image, tf.transpose(idx3))\n",
    "\n",
    "    return tf.reshape(d, [DIM, DIM, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-broughton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "git-nbdiffdriver diff: 1: git-nbdiffdriver: not found\n",
      "fatal: external diff died, stopping at training/train.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Desktop/Mushroom-Classifier/notebooks/wandb/run-20231029_042419-b30y02g2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/b30y02g2' target=\"_blank\">charmed-fog-142</a></strong> to <a href='https://wandb.ai/g-broughton/Mushroom-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/Mushroom-Classifier' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/Mushroom-Classifier/runs/b30y02g2' target=\"_blank\">https://wandb.ai/g-broughton/Mushroom-Classifier/runs/b30y02g2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact Mushroom-Classifier:latest, 2262.80MB. 5 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n",
      "Done. 0:0:3.8\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Mushroom-Classifier\", job_type=\"testing\", )\n",
    "path = wandb.use_artifact(\"g-broughton/model-registry/Mushroom-Classifier:latest\", type=\"model\").download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/broug/mambaforge/envs/ds/lib/python3.11/site-packages/keras/src/layers/core/lambda_layer.py:327: UserWarning: src.training.NN is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PATH_SELECT = {\n",
    "    192: f\"{environ['GCS_REPO']}/tfrecords-jpeg-192x192\",\n",
    "    224: f\"{environ['GCS_REPO']}/tfrecords-jpeg-224x224v2\",\n",
    "    384: f\"{environ['GCS_REPO']}/tfrecords-jpeg-384x384\",\n",
    "    512: f\"{environ['GCS_REPO']}/tfrecords-jpeg-512x512\",\n",
    "}\n",
    "GCS_PATH = GCS_PATH_SELECT[CFG.IMAGE_SIZE[0]]\n",
    "\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(f\"{GCS_PATH}/val*.tfrec\")\n",
    "\n",
    "# NUM_TRAINING_IMAGES = tr_fn.count_data_items(TRAINING_FILENAMES)\n",
    "# NUM_VALIDATION_IMAGES = tr_fn.count_data_items(VALIDATION_FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mush-img-repo/tfrecords-jpeg-224x224v2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "preds_l = []\n",
    "labels_l = []\n",
    "for images, labels in get_validation_dataset(VALIDATION_FILENAMES, CFG).take(1):\n",
    "    preds = model.predict(images)\n",
    "    preds_l.append(preds)\n",
    "    labels_l.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.0538904e-12, 1.3545815e-12, 1.7053411e-11, ..., 3.4159637e-11,\n",
       "        7.8716643e-12, 5.5715502e-12],\n",
       "       [3.5986499e-08, 9.1118423e-08, 5.2418478e-09, ..., 7.2495165e-09,\n",
       "        1.2011550e-09, 3.2498737e-09],\n",
       "       [5.4941909e-11, 1.5688406e-10, 5.3636410e-12, ..., 2.7036511e-09,\n",
       "        2.9166478e-10, 4.7350941e-08],\n",
       "       ...,\n",
       "       [1.6051135e-10, 4.8369749e-08, 2.9984335e-09, ..., 2.6298627e-08,\n",
       "        1.2206925e-08, 7.5581198e-08],\n",
       "       [7.4830867e-05, 1.3993742e-08, 4.5303295e-06, ..., 1.1116118e-06,\n",
       "        3.9574690e-08, 3.5087339e-07],\n",
       "       [1.0749997e-06, 5.3991571e-09, 3.8924462e-08, ..., 2.1432578e-09,\n",
       "        9.2097252e-10, 5.6187344e-09]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999547, 0.9998259 , 0.99997485, 0.9859897 , 0.48543844,\n",
       "        0.99956673, 0.9739571 , 0.652332  ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(preds_l, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
