{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import exifread\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_exif(url):\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Ensure that the request was successful\n",
    "\n",
    "    # Save the image to a BytesIO object (in-memory file)\n",
    "    img_file = io.BytesIO(response.content)\n",
    "\n",
    "    # Use exifread to extract EXIF data\n",
    "    tags = exifread.process_file(img_file)\n",
    "\n",
    "    # For further processing, load the image using PIL\n",
    "    img_file.seek(0)  # Reset file pointer to the beginning\n",
    "    img = Image.open(img_file)\n",
    "\n",
    "    return img, tags\n",
    "\n",
    "def get_image_dimensions(exif_data):\n",
    "    # The tags can have different names depending on the camera and the file format\n",
    "    width_tag = exif_data.get('EXIF ExifImageWidth') or exif_data.get('Image ImageWidth')\n",
    "    height_tag = exif_data.get('EXIF ExifImageLength') or exif_data.get('Image ImageLength')\n",
    "\n",
    "    if width_tag and height_tag:\n",
    "        width = width_tag.values[0]\n",
    "        height = height_tag.values[0]\n",
    "        return width, height\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def hash_phone_number(phone_number):\n",
    "    # Create a new SHA-256 hash object\n",
    "    hasher = hashlib.sha256()\n",
    "    \n",
    "    # Update the hash object with the phone number\n",
    "    # Encode the phone number to bytes\n",
    "    hasher.update(phone_number.encode('utf-8'))\n",
    "\n",
    "    # Return the hexadecimal representation of the digest\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def get_decimal_from_dms(dms, ref):\n",
    "    degrees = dms[0]\n",
    "    minutes = dms[1] / 60.0\n",
    "    seconds = dms[2] / 3600.0\n",
    "\n",
    "    if ref in ['S', 'W']:\n",
    "        degrees = -degrees\n",
    "        minutes = -minutes\n",
    "        seconds = -seconds\n",
    "\n",
    "    return degrees + minutes + seconds\n",
    "\n",
    "def get_gps_coords(exif_data):\n",
    "    lat = None\n",
    "    lon = None\n",
    "\n",
    "    gps_latitude = exif_data.get('GPS GPSLatitude')\n",
    "    gps_latitude_ref = exif_data.get('GPS GPSLatitudeRef')\n",
    "    gps_longitude = exif_data.get('GPS GPSLongitude')\n",
    "    gps_longitude_ref = exif_data.get('GPS GPSLongitudeRef')\n",
    "\n",
    "    if gps_latitude and gps_latitude_ref and gps_longitude and gps_longitude_ref:\n",
    "        lat = get_decimal_from_dms(gps_latitude.values, gps_latitude_ref.values)\n",
    "        lon = get_decimal_from_dms(gps_longitude.values, gps_longitude_ref.values)\n",
    "\n",
    "    return lat, lon\n",
    "\n",
    "def get_date_and_gps_coords(exif_data):\n",
    "    date_taken_str = str(exif_data.get('EXIF DateTimeOriginal'))\n",
    "    gps_coords = get_gps_coords(exif_data)  # Assuming this function returns the GPS coordinates correctly\n",
    "\n",
    "    # Parse the date string and convert it to a datetime object\n",
    "    if date_taken_str:\n",
    "        # Ensure the date string is in the expected format\n",
    "        matched = re.match(r'\\d{4}:\\d{2}:\\d{2} \\d{2}:\\d{2}:\\d{2}', date_taken_str)\n",
    "        if matched:\n",
    "            date_taken = datetime.strptime(matched.group(), '%Y:%m:%d %H:%M:%S')\n",
    "            day_of_year = date_taken.timetuple().tm_yday\n",
    "            month = date_taken.month\n",
    "            return day_of_year, month, gps_coords\n",
    "        else:\n",
    "            # Handle unexpected date format\n",
    "            print(\"Date format is not as expected.\")\n",
    "            return None, None, gps_coords\n",
    "    else:\n",
    "        # Handle cases where date is not present\n",
    "        print(\"No date information found.\")\n",
    "        return None, None, gps_coords\n",
    "\n",
    "def initialize_database(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS image_data (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            hash_id TEXT,\n",
    "            image BLOB,\n",
    "            height INTEGER,\n",
    "            width INTEGER,\n",
    "            latitude REAL,\n",
    "            longitude REAL,\n",
    "            day_of_year INTEGER,\n",
    "            month INTEGER\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_image_data(\n",
    "        db_name: str, \n",
    "        hash_id: str, \n",
    "        img: Image, \n",
    "        height: int, \n",
    "        width: int, \n",
    "        latitude: float, \n",
    "        longitude: float, \n",
    "        day_of_year: int,\n",
    "        month: int\n",
    "    ):\n",
    "    # Convert PIL image to binary format\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format=img.format)\n",
    "    img_blob = img_byte_arr.getvalue()\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect('image_data.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Insert the data\n",
    "    cursor.execute('''\n",
    "        INSERT INTO image_data (hash_id, image, height, width, latitude, longitude, day_of_year, month)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (hash_id, img_blob, height, width, latitude, longitude, day_of_year, month))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_exif_data(exif_data):\n",
    "    width, height = get_image_dimensions(exif_data)\n",
    "    day_of_year, month, gps_coords = get_date_and_gps_coords(exif_data)\n",
    "\n",
    "    return width, height, day_of_year, month, gps_coords\n",
    "\n",
    "def process_image(url, hash_id):\n",
    "    img, exif_data = get_image_and_exif(url)\n",
    "    width, height, day_of_year, month, gps_coords = get_exif_data(exif_data)\n",
    "    insert_image_data('images.db', hash_id, img, height, width, gps_coords[0], gps_coords[1], day_of_year, month)\n",
    "    img = tf.image.resize(tf.convert_to_tensor(np.array(img)), [224, 224]) #CFG.CROP_SIZE)\n",
    "    return img, gps_coords[0], gps_coords[1],  day_of_year, month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_feats(meta):\n",
    "    days = meta[2]\n",
    "    months = meta[3]\n",
    "\n",
    "    months_in_year = 12\n",
    "    days_in_year = 365\n",
    "\n",
    "    day_sin = tf.math.sin(days * (2 * np.pi / days_in_year))\n",
    "    day_cos = tf.math.cos(days * (2 * np.pi / days_in_year))\n",
    "\n",
    "    month_sin = tf.math.sin(months * (2 * np.pi / months_in_year))\n",
    "    month_cos = tf.math.cos(months * (2 * np.pi / months_in_year))\n",
    "    return tf.stack([day_sin, day_cos, month_sin, month_cos], axis=-1)\n",
    "\n",
    "def map_coordinates_to_grid(meta, grid_size=(100, 100), normalize=True):\n",
    "    latitudes, longitudes = meta[0], meta[1]\n",
    "\n",
    "    normalized_lat = (latitudes + 90) / 180\n",
    "    normalized_lon = (longitudes + 180) / 360\n",
    "\n",
    "    grid_x = tf.cast(normalized_lon * grid_size[1], tf.float32)\n",
    "    grid_y = tf.cast(normalized_lat * grid_size[0], tf.float32)\n",
    "\n",
    "    grid_x = tf.clip_by_value(grid_x, 0, grid_size[1] - 1)\n",
    "    grid_y = tf.clip_by_value(grid_y, 0, grid_size[0] - 1)\n",
    "\n",
    "    if normalize:\n",
    "        # Normalize to 0-1 range\n",
    "        grid_x = grid_x / (grid_size[1] - 1)\n",
    "        grid_y = grid_y / (grid_size[0] - 1)\n",
    "\n",
    "    return tf.stack([grid_x, grid_y], axis=-1)\n",
    "\n",
    "@tf.function\n",
    "def process_meta(meta):\n",
    "    date_feats = get_date_feats(meta)\n",
    "    gps_feat = map_coordinates_to_grid(meta)\n",
    "    meta = tf.concat([date_feats, gps_feat], axis=-1)\n",
    "    return meta\n",
    "\n",
    "def load_dataset(url_list, hash_id): #, CFG):\n",
    "    imgs_exif_list = list(\n",
    "        map(\n",
    "            lambda x: process_image(*x), #, CFG),\n",
    "            zip(url_list, [hash_id] * len(url_list))\n",
    "        )\n",
    "    )\n",
    "    img = tf.data.Dataset.from_tensor_slices([x[0] for x in imgs_exif_list])\n",
    "    meta = tf.data.Dataset.from_tensor_slices([x[1:] for x in imgs_exif_list])\n",
    "    ds = tf.data.Dataset.zip((img, meta))\n",
    "    ds = ds.map(lambda x, y: (x, process_meta(y)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(1)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_database(\"image_data.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date format is not as expected.\n",
      "Date format is not as expected.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\util\\structure.py:104\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    105\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\util\\structure.py:507\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    504\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[0;32m    505\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[1;32m--> 507\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    508\u001b[0m     element,\n\u001b[0;32m    509\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a `TypeSpec` for [(None, None, None, None), (None, None, None, None)] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\broug\\Desktop\\Mushroom-Classifier\\notebooks\\09.EXIF-EDA.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hash_id \u001b[39m=\u001b[39m hash_phone_number(\u001b[39mstr\u001b[39m(number))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m url_list \u001b[39m=\u001b[39m url_list\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ds \u001b[39m=\u001b[39m load_dataset(url_list, hash_id) \u001b[39m#, CFG)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\broug\\Desktop\\Mushroom-Classifier\\notebooks\\09.EXIF-EDA.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m imgs_exif_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mmap\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         \u001b[39mlambda\u001b[39;00m x: process_image(\u001b[39m*\u001b[39mx), \u001b[39m#, CFG),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         \u001b[39mzip\u001b[39m(url_list, [hash_id] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(url_list))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices([x[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m imgs_exif_list])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m meta \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices([x[\u001b[39m1\u001b[39;49m:] \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m imgs_exif_list])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip((img, meta))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/broug/Desktop/Mushroom-Classifier/notebooks/09.EXIF-EDA.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (x, process_meta(y)), num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mAUTOTUNE)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:831\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[39m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    829\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_tensor_slices_op\n\u001b[1;32m--> 831\u001b[0m \u001b[39mreturn\u001b[39;00m from_tensor_slices_op\u001b[39m.\u001b[39;49m_from_tensor_slices(tensors, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m   \u001b[39mreturn\u001b[39;00m _TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[0;32m     34\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m     35\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\util\\structure.py:109\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    104\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 109\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m   \u001b[39m# To avoid a circular dependency between dataset_ops and structure,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m   \u001b[39m# we check the class name instead of using `isinstance`.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m   \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDatasetSpec\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:1443\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[39m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m preferred_dtype \u001b[39m=\u001b[39m preferred_dtype \u001b[39mor\u001b[39;00m dtype_hint\n\u001b[1;32m-> 1443\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[0;32m   1444\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:324\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    322\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    323\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> 324\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    168\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    264\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:275\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    274\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 275\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    277\u001b[0m const_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39m_create_graph_constant(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    279\u001b[0m )\n\u001b[0;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    284\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    286\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "url_list = ['https://api.twilio.com/2010-04-01/Accounts/AC2fe2a3bdeddcd03eaabc4f99f6717c9d/Messages/MM4c912da342595672a27c13e914286095/Media/ME0dfacbd9c422ee55ee44058e56073445', 'https://api.twilio.com/2010-04-01/Accounts/AC2fe2a3bdeddcd03eaabc4f99f6717c9d/Messages/MMf47897b4a52a4defbea1d70afca5c9a1/Media/ME927d17cb31f15cf5536469e7a6ddbfe2']\n",
    "number = 2508808120\n",
    "hash_id = hash_phone_number(str(number))\n",
    "url_list = url_list\n",
    "\n",
    "ds = load_dataset(url_list, hash_id) #, CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image ExifOffset': (0x8769) Long=26 @ 18,\n",
       " 'EXIF ColorSpace': (0xA001) Short=sRGB @ 36,\n",
       " 'EXIF ExifImageWidth': (0xA002) Long=2250 @ 48,\n",
       " 'EXIF ExifImageLength': (0xA003) Long=3000 @ 60}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(url_list[0], stream=True)\n",
    "response.raise_for_status()  # Ensure that the request was successful\n",
    "\n",
    "# Save the image to a BytesIO object (in-memory file)\n",
    "img_file = io.BytesIO(response.content)\n",
    "\n",
    "# Use exifread to extract EXIF data\n",
    "tags = exifread.process_file(img_file)\n",
    "tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3024x4032>,\n",
       " {'Image Orientation': (0x0112) Short=Horizontal (normal) @ 18,\n",
       "  'Image XResolution': (0x011A) Ratio=72 @ 74,\n",
       "  'Image YResolution': (0x011B) Ratio=72 @ 82,\n",
       "  'Image ResolutionUnit': (0x0128) Short=Pixels/Inch @ 54,\n",
       "  'Image ExifOffset': (0x8769) Long=90 @ 66,\n",
       "  'EXIF ColorSpace': (0xA001) Short=Uncalibrated @ 100,\n",
       "  'EXIF ExifImageWidth': (0xA002) Long=4032 @ 112,\n",
       "  'EXIF ExifImageLength': (0xA003) Long=3024 @ 124})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, tags = get_image_and_exif(\"https://images.mushroomobserver.org/orig/1625008.jpg\")\n",
    "img, tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = '../training/data/1625361.jpg'\n",
    "# date, gps_coords = get_date_and_gps_coords(image_path)\n",
    "# print(f\"Date Taken: {date}, GPS Coordinates: {gps_coords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(image_path, 'rb') as image_file:\n",
    "    exif_data = exifread.process_file(image_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
